{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inicialización"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Carga de datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('../datasets/users_behavior.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploración y corrección inicial de datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 3214 entries, 0 to 3213\n",
      "Data columns (total 5 columns):\n",
      " #   Column    Non-Null Count  Dtype  \n",
      "---  ------    --------------  -----  \n",
      " 0   calls     3214 non-null   float64\n",
      " 1   minutes   3214 non-null   float64\n",
      " 2   messages  3214 non-null   float64\n",
      " 3   mb_used   3214 non-null   float64\n",
      " 4   is_ultra  3214 non-null   int64  \n",
      "dtypes: float64(4), int64(1)\n",
      "memory usage: 125.7 KB\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>calls</th>\n",
       "      <th>minutes</th>\n",
       "      <th>messages</th>\n",
       "      <th>mb_used</th>\n",
       "      <th>is_ultra</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>40.0</td>\n",
       "      <td>311.90</td>\n",
       "      <td>83.0</td>\n",
       "      <td>19915.42</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>85.0</td>\n",
       "      <td>516.75</td>\n",
       "      <td>56.0</td>\n",
       "      <td>22696.96</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>77.0</td>\n",
       "      <td>467.66</td>\n",
       "      <td>86.0</td>\n",
       "      <td>21060.45</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>106.0</td>\n",
       "      <td>745.53</td>\n",
       "      <td>81.0</td>\n",
       "      <td>8437.39</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>66.0</td>\n",
       "      <td>418.74</td>\n",
       "      <td>1.0</td>\n",
       "      <td>14502.75</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3209</th>\n",
       "      <td>122.0</td>\n",
       "      <td>910.98</td>\n",
       "      <td>20.0</td>\n",
       "      <td>35124.90</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3210</th>\n",
       "      <td>25.0</td>\n",
       "      <td>190.36</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3275.61</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3211</th>\n",
       "      <td>97.0</td>\n",
       "      <td>634.44</td>\n",
       "      <td>70.0</td>\n",
       "      <td>13974.06</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3212</th>\n",
       "      <td>64.0</td>\n",
       "      <td>462.32</td>\n",
       "      <td>90.0</td>\n",
       "      <td>31239.78</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3213</th>\n",
       "      <td>80.0</td>\n",
       "      <td>566.09</td>\n",
       "      <td>6.0</td>\n",
       "      <td>29480.52</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3214 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      calls  minutes  messages   mb_used  is_ultra\n",
       "0      40.0   311.90      83.0  19915.42         0\n",
       "1      85.0   516.75      56.0  22696.96         0\n",
       "2      77.0   467.66      86.0  21060.45         0\n",
       "3     106.0   745.53      81.0   8437.39         1\n",
       "4      66.0   418.74       1.0  14502.75         0\n",
       "...     ...      ...       ...       ...       ...\n",
       "3209  122.0   910.98      20.0  35124.90         1\n",
       "3210   25.0   190.36       0.0   3275.61         0\n",
       "3211   97.0   634.44      70.0  13974.06         0\n",
       "3212   64.0   462.32      90.0  31239.78         0\n",
       "3213   80.0   566.09       6.0  29480.52         1\n",
       "\n",
       "[3214 rows x 5 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Primero le demos un vistazo al df\n",
    "df.info()\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "De lo que podemos observar las columnas son:\n",
    "- `сalls` — número de llamadas\n",
    "- `minutes` — duración total de la llamada en minutos\n",
    "- `messages` — número de mensajes de texto\n",
    "- `mb_used` — Tráfico de Internet utilizado en MB\n",
    "- `is_ultra` — plan para el mes actual (Ultra - 1, Smart - 0)\n",
    "  \n",
    "Podemos notar un leve problema con el tipo de datos en nuestra tabla ya que las columnas `calls y messages` son *float* cuando deberian ser *int*. Tambien notamos como `is_ultra` es *int* cuando debería ser *boolean*.\n",
    "  \n",
    "\n",
    "Para nuestra suerte no vemos ningún valor ausente lo que facilita nuestro trabajo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Por ultimo veamos si hay algun duplicado en la tabla\n",
    "df.duplicated().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Con solo un problema fácil de tratar vamos a encargarnos de él y seguir con el proyecto."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simplemente reemplazamos los valores con la funcion astype\n",
    "df = df.astype({'calls': int, 'messages': int, 'is_ultra': bool})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Segmentación de datos\n",
    "  \n",
    "Dado que nuestro objetivo es predecir cual de los planes es más apropiado según el comportamiento del cliente es evidente que nuestro *target* es la columna `is_ultra` mientras que el resto de las columnas son las *features* sobre las cuales el modelo basará su predicción.\n",
    "  \n",
    "Vamos a usar la función `GridSearchCV` para la optimización de hiperparámetros la que nos permitirá reducir el sobreajuste de  nuestros modelos. Para lograr de mejor manera eso, vamos a dividir los datos en 2 partes: entrenamiento (75%) y pruebas (25%). \n",
    "  \n",
    "De no usar `GridSearchCV` si habría que separar el dataset en 3 partes para tener un conjunto de validación."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Primero separamos el df en las features y los target\n",
    "features = df.drop(['is_ultra'], axis=1)\n",
    "target = df['is_ultra']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Primero segmentamos los datos en los datos de entrenamiento y los datos de validacion/prueba\n",
    "features_train, features_test, target_train, target_test = train_test_split(\n",
    "    features, target, test_size=0.25, random_state=56982)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Con nuestros datos segmentados de la forma correcta podemos proceder a entrenar nuestro modelo, aunque ahora nos vamos a enfocar en ver cual es el mejor modelo y cuales son los mejores hiperparámetros para lograr los mejores resultados."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prueba de modelos e hiperparámetros\n",
    "  \n",
    "En éste caso nos encontramos con una problemática de clasificación por lo que vamos a probar 3 modelos:\n",
    "  \n",
    "- Modelo de árbol de decisiones (DecisionTreeClassifier)\n",
    "- Modelo de bosque alteatorio (RandomForestClassifier)\n",
    "- Modelo de regresión logística (LogisticRegression)\n",
    "  \n",
    "Para poder refinar los diferentes modelos, obtener el mejor de cada uno y decidir cual será el modelo final vamos a utilizar el dataset de validación. Sin ir más lejos, vamos allá!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modelo de árbol de decisiones\n",
    "  \n",
    "Vamos a empezar con el árbol de decisiones ya que si bien el bosque aleatorio fabrica multiples árboles, nosotros vamos a poder obtener un conocimiento previo sobre cuales tipos de árboles presentan mejores predicciones para nuestro caso.\n",
    "  \n",
    "En vez de fabricar un nido enorme de bucles para probar y encontrar la mejor combinación de hiperparámetros, vamos a usar la funcion GridSearchCV. Vamos a explicar el motivo de cada parámetro que vamos a elegir.\n",
    "- `criterion`: Vamos a elegir unicamente *entropy* ya que *gini* sirve principalemente si tenemos valores ausentes, que no es nuestro caso.\n",
    "- `max_depth`: Vamos a probar profundidades del 1 al 10 ya que extendernos más allá de eso sería poco útil ya que nos puede traer sobreajuste y la misma función descarta automáticamente los modelos sobreajustados.\n",
    "- `min_samples_split`: En éste caso probamos un rango de 4 a 12. \n",
    "- `min_samples_leaf`: En este caso es mejor mantenernos en valores aún más bajos para evitar de vuelta un modelo demasiado generalizado.\n",
    "- `max_features`: Vamos a probar las funciones que nos permite *DecisionTreeClassifier* junto con la default que es *None*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Primero definimos los parametros que vamos a probar\n",
    "params = {\n",
    "    'random_state':[17203],\n",
    "    'criterion':['entropy'],\n",
    "    'max_depth':tuple(range(3,13)),\n",
    "    'min_samples_split':tuple(range(2,11)),\n",
    "    'min_samples_leaf':tuple(range(1,6)),\n",
    "    'max_features':('auto', 'sqrt', 'log2',None)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# De ahí guardamos el grid en su variable para posterior trabajo\n",
    "tree_grid = GridSearchCV(DecisionTreeClassifier(), param_grid= params, cv= 10, verbose= 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 10 folds for each of 1800 candidates, totalling 18000 fits\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=10, estimator=DecisionTreeClassifier(),\n",
       "             param_grid={'criterion': ['entropy'],\n",
       "                         'max_depth': (3, 4, 5, 6, 7, 8, 9, 10, 11, 12),\n",
       "                         'max_features': ('auto', 'sqrt', 'log2', None),\n",
       "                         'min_samples_leaf': (1, 2, 3, 4, 5),\n",
       "                         'min_samples_split': (2, 3, 4, 5, 6, 7, 8, 9, 10),\n",
       "                         'random_state': [17203]},\n",
       "             verbose=1)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Entrenamos la grid para encontrar las mejores combinaciónes\n",
    "tree_grid.fit(features_train, target_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DecisionTreeClassifier(criterion='entropy', max_depth=5, max_features='auto',\n",
       "                       min_samples_leaf=5, random_state=17203)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Finalmente le pedimos que nos de la combinación secreta\n",
    "tree_grid.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creamos entonces el modelo\n",
    "tree_model = tree_grid.best_estimator_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora vamos a observar como es la exactitud de nuestro modelo frente al conjunto de entrenamiento y al de prueba."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exactitud con el conjunto de datos de entrenamiento: 0.8141078838174274\n",
      "Exactitud con el conjunto de datos de prueba: 0.777363184079602\n"
     ]
    }
   ],
   "source": [
    "for feature, target, text in zip([features_train, features_test],\n",
    "                                 [target_train, target_test],\n",
    "                                 ['entrenamiento','prueba']):\n",
    "    prediction = tree_model.predict(feature)\n",
    "    \n",
    "    accuracy = accuracy_score(target, prediction)\n",
    "    \n",
    "    print(f'Exactitud con el conjunto de datos de {text}: {accuracy}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bueno, esos resultados no son los mejores pero al menos pasan el umbral. No esperaba el mejor rendimiento dado que estamos trabajando con éste modelo que es relativamente simple. De todas formas vemos que más allá de usar la *grid* nos encontramos con un sobreajuste destacable.\n",
    "  \n",
    "De todas formas, pude notar que la *grid* no modificó el valor de *min_samples_split* y tomó el valor más alto de *min_samples_leaf*. Por lo tanto voy a llamar de vuelta a la función pero ésta vez le voy a pasar un rango de valores más alto en esos dos hiperparámetros."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definimos los parametros que vamos a probar\n",
    "params_2 = {\n",
    "    'random_state':[17203],\n",
    "    'criterion':['entropy'],\n",
    "    'max_depth':tuple(range(3,13)),\n",
    "    'min_samples_split':tuple(range(5,14)),\n",
    "    'min_samples_leaf':tuple(range(5,10)),\n",
    "    'max_features':('auto', 'sqrt', 'log2',None)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# De ahí guardamos el grid en su variable para posterior trabajo\n",
    "tree_grid_2 = GridSearchCV(DecisionTreeClassifier(), param_grid= params_2, cv= 10, verbose= 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 10 folds for each of 1800 candidates, totalling 18000 fits\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=10, estimator=DecisionTreeClassifier(),\n",
       "             param_grid={'criterion': ['entropy'],\n",
       "                         'max_depth': (3, 4, 5, 6, 7, 8, 9, 10, 11, 12),\n",
       "                         'max_features': ('auto', 'sqrt', 'log2', None),\n",
       "                         'min_samples_leaf': (5, 6, 7, 8, 9),\n",
       "                         'min_samples_split': (5, 6, 7, 8, 9, 10, 11, 12, 13),\n",
       "                         'random_state': [17203]},\n",
       "             verbose=1)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Veamos si ahora encontramos otras condiciones\n",
    "tree_grid_2.fit(features_train, target_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DecisionTreeClassifier(criterion='entropy', max_depth=5, max_features='auto',\n",
       "                       min_samples_leaf=5, min_samples_split=12,\n",
       "                       random_state=17203)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Cambio el mejor modelo?\n",
    "tree_grid_2.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Al parecer min_samples_split es mejor en 12, guardemos para evaluar como cambio\n",
    "tree_model_2 = tree_grid_2.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DecisionTreeClassifier(criterion='entropy', max_depth=5, max_features='auto',\n",
       "                       min_samples_leaf=5, min_samples_split=12,\n",
       "                       random_state=17203)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tree_model_2.fit(features_train, target_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exactitud con el conjunto de datos de entrenamiento: 0.8141078838174274\n",
      "Exactitud con el conjunto de datos de prueba: 0.777363184079602\n"
     ]
    }
   ],
   "source": [
    "for feature_2, target_2, text_2 in zip([features_train, features_test],\n",
    "                                 [target_train, target_test],\n",
    "                                 ['entrenamiento','prueba']):\n",
    "    prediction_2 = tree_model_2.predict(feature_2)\n",
    "    \n",
    "    accuracy_2 = accuracy_score(target_2, prediction_2)\n",
    "    \n",
    "    print(f'Exactitud con el conjunto de datos de {text_2}: {accuracy_2}')\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No voy a mentir, al ver ese resultado la primera vez pensé que simplemente había un problema con los nombres de las variables que me llebavan a llamar los valores anteriores y no los nuevos. Ante ese pensamiento simplemente agregué un  \"_2\"  a todo para asegurarme y mira mi sorpresa cuando veo que efectivamente el modelo sigue igual (hasta incluso modifiqué los nombres de las variables internas del *for* de pura paranoia). Por más extraño que me parezca, nos encontramos con que en nuestro modelo (y nuestros datos) el hiperparámetro *min_samples_split* no es muy influyente.\n",
    "  \n",
    "Tras visualizar los árboles de mis modelos encontré el motivo por el cual los resultados terminaron siendo iguales. Simplemente sucede que la cantidad de muestras en cada split nunca llega a superar las 12 y ni cerca.\n",
    "  \n",
    "De todas formas, algo que llegué a entender es que por más que me esfuerce en pulir el árbol de decisiones, parece que éste simplemente no pareciera ser el mejor. Habría que probar que resultados obtenemos con los otros modelos para sacar una conclusión definitiva respecto al tema."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modelo de bosque aleatorio\n",
    "  \n",
    "Basado en el árbol de decisión pero escalado a proporciones mayores y al hacer eso se le puede introducir aleatoriedad lo que le permite obtener una mayor adaptabilidad ante datos extraños.\n",
    "  \n",
    "El procedimiento será el mismo que con el árbol, usaremos la función GridSearchCV para obtener la mejor combinación de hiperparámetros y posteriormente los pondremos a prueba."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Primero determinaremos sobre cuales hiperparámetros vamos a trabajar\n",
    "params = {\n",
    "    'n_estimators': [50,75,100],\n",
    "    'criterion': ['entropy'],\n",
    "    'max_depth': list(range(2,7)),\n",
    "    'min_samples_split': list(range(2,10)),\n",
    "    'min_samples_leaf': list(range(1,6)),\n",
    "    'max_features': ['sqrt', 'log2', None],\n",
    "    'random_state': [17203]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cargamos el grid en una variable\n",
    "forest_grid = GridSearchCV(RandomForestClassifier(), param_grid= params, cv= 5, verbose= 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 1800 candidates, totalling 9000 fits\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=5, estimator=RandomForestClassifier(),\n",
       "             param_grid={'criterion': ['entropy'], 'max_depth': [2, 3, 4, 5, 6],\n",
       "                         'max_features': ['sqrt', 'log2', None],\n",
       "                         'min_samples_leaf': [1, 2, 3, 4, 5],\n",
       "                         'min_samples_split': [2, 3, 4, 5, 6, 7, 8, 9],\n",
       "                         'n_estimators': [50, 75, 100],\n",
       "                         'random_state': [17203]},\n",
       "             verbose=1)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Buscamos el mejor conjunto de h-parametros para nuestros datos\n",
    "forest_grid.fit(features_train, target_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(criterion='entropy', max_depth=6, max_features='sqrt',\n",
       "                       min_samples_split=8, n_estimators=75,\n",
       "                       random_state=17203)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Finalmente vemos cual es\n",
    "forest_grid.best_estimator_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nos detengamos a analizar el modelo obtenido de la grid. Lo primero que quiero observar es si alguno de los h-parametros tomó el valor más alto del rango que le dí. Al ver con ese foco vemos que solo *max_depth* cumple con esa condición, mientras tanto vemos que *min_samples_leaf* ni siquiera fue especificado por lo que podemos deducir que no tiene un impacto grande en nuestro modelo con nuestros datos. Antes de probar otra combinación de h-parametros, ya que tarda tanto en ejecutarse, vamos a ver la exactitud del modelo en cuestión."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cargamos el modelo en una variable\n",
    "forest_model = forest_grid.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(criterion='entropy', max_depth=6, max_features='sqrt',\n",
       "                       min_samples_split=8, n_estimators=75,\n",
       "                       random_state=17203)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Lo entrenamos\n",
    "forest_model.fit(features_train, target_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exactitud con el conjunto de datos de entrenamiento: 0.8327800829875519\n",
      "Exactitud con el conjunto de datos de prueba: 0.7761194029850746\n"
     ]
    }
   ],
   "source": [
    "# Y finalmente lo ponemos a prueba\n",
    "for feature, target, text in zip([features_train, features_test],\n",
    "                                 [target_train, target_test],\n",
    "                                 ['entrenamiento','prueba']):\n",
    "    \n",
    "    prediction = forest_model.predict(feature)\n",
    "    \n",
    "    accuracy = accuracy_score(target, prediction)\n",
    "    \n",
    "    print(f'Exactitud con el conjunto de datos de {text}: {accuracy}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora vamos a efectuar las correcciónes que destacamos previamente y vamos a hacer un *grid* nuevo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Primero designamos los parámetros\n",
    "params = {\n",
    "    'n_estimators': [50,75,100],\n",
    "    'criterion': ['entropy'],\n",
    "    'max_depth': list(range(6,13)),\n",
    "    'min_samples_split': list(range(2,10)),\n",
    "    'max_features': ['sqrt', 'log2', None,2],\n",
    "    'random_state': [17203]\n",
    "}\n",
    "\n",
    "# De ahí creamos el grid con los nuebos parametros\n",
    "forest_grid_2 = GridSearchCV(RandomForestClassifier(), param_grid= params, cv= 8, verbose= 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 8 folds for each of 672 candidates, totalling 5376 fits\n"
     ]
    }
   ],
   "source": [
    "# Ejecutamos el fit en el grid para nuestros datos\n",
    "forest_grid_2.fit(features_train, target_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Y veamos que nos dió\n",
    "forest_grid_2.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cargamos el nuevo modelo en otra variable\n",
    "forest_model_2 = forest_grid_2.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Entrenamos el nuevo modelo\n",
    "forest_model_2.fit(features_train, target_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Y vemos la exactitud con los conjuntos de prueba y entrenamiento\n",
    "for feature, target, text in zip([features_train, features_test],\n",
    "                                 [target_train, target_test],\n",
    "                                 ['entrenamiento','prueba']):\n",
    "    \n",
    "    prediction = forest_model_2.predict(feature)\n",
    "    \n",
    "    accuracy = accuracy_score(target, prediction)\n",
    "    \n",
    "    print(f'Exactitud con el conjunto de datos de {text}: {accuracy}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Okay, dejemos bien escrito los valores para poder compararlos como se debe:\n",
    "- Modelo 1: \n",
    "    - Exactitud con el conjunto de datos de entrenamiento: 0.83278\n",
    "    - Exactitud con el conjunto de datos de prueba: 0.77611\n",
    "    - Diferencia entre los dos: 0.05667\n",
    "  \n",
    "- Modelo 2:\n",
    "    - Exactitud con el conjunto de datos de entrenamiento: 0.85518\n",
    "    - Exactitud con el conjunto de datos de prueba: 0.77860\n",
    "    - Diferencia entre los dos: 0.07658 \n",
    "\n",
    "Al ver los datos uno encima del otro nos facilita ver los cambios que tienen los resultados. Podemos ver como en el segundo modelo se alcanzó en ambos casos una mayor exactitud que en el primero, aunque cabe destacar que tambien se aumentó la distancia entre la exactitud de los datos de entrenamiento y los de prueba. Eso nos puede indicar que el segundo modelo posiblemente esté un poco mas sobreajustado que el primero, aunque los dos están sobreajustados hasta cierto punto.\n",
    "  \n",
    "A diferencia del árbol de decisiones, acá si son diferentes los resultados de los modelos por lo que tengo que elegir uno para comparar al final. Para éste caso me encuentro en la disyuntiva ya que el objetivo de éste estudio es encontrar el modelo con la mayor exactitud posible, que en nuestro caso sería el segundo. Pero el problema que se me viene a la mente es que según los resultados que tenemos arriba el segundo modelo tiene mayor exactitud en el conjunto de prueba pero a la vez posee una mayor diferencia con la exactitud del conjunto de entrenamiento lo que me indica que está más sobreajustado. De todas formas, las reglas son las reglas y para respetarlas debemos elegir entonces el segundo modelo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modelo de regresión logística\n",
    "  \n",
    "Mantendremos el mismo modus operandi que tuvimos con los otros dos modelos y usaremos un grid para encontrar la mejor combinación de h-parámetros para nuestro modelo acorde a nuestros datos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Primero vamos a determinar los h-parámetros que vamos a probar\n",
    "params = {\n",
    "    'C': np.logspace(-4,4,30),\n",
    "    'solver': ['lbfgs', 'liblinear'],\n",
    "    'max_iter': list(range(1000,2100,50)),\n",
    "    'random_state': [17203]\n",
    "}\n",
    "\n",
    "logistic_grid = GridSearchCV(LogisticRegression(), param_grid= params, cv= 10, verbose= 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Con el grid establecido vamos a probar cuales son los mejores h-parametros\n",
    "logistic_grid.fit(features_train, target_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vemos entonces el mejor resultado\n",
    "logistic_grid.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lo guardamos una variable\n",
    "logistic_model = logistic_grid.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Y lo entrenamos\n",
    "logistic_model.fit(features_train, target_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finalmente vemos la exactitud con los conjuntos de prueba y entrenamiento\n",
    "for feature, target, text in zip([features_train, features_test],\n",
    "                                 [target_train, target_test],\n",
    "                                 ['entrenamiento','prueba']):\n",
    "    \n",
    "    prediction = logistic_model.predict(feature)\n",
    "    \n",
    "    accuracy = accuracy_score(target, prediction)\n",
    "    \n",
    "    print(f'Exactitud con el conjunto de datos de {text}: {accuracy}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Un poco me esperaba ésto. El modelo de regresión logística es caracterizado por ser eficiente y rápido pero no muy preciso. Una cosa que tambien se puede afirmar sobre éste modelo es que lidia muy bien con el sobreajuste y lo vemos claramente ya que presenta valores muy similares tanto en los datos de entrenamiento como en los de prueba. En resumen, es un buen modelo para algo que no requiera exactitud... triste que no es nuestro caso. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Elección final y prueba de cordura\n",
    "  \n",
    "Ahora llegó el momento de elegir al ganador de éste evento. Primero, recapitulemos los resultados de cada modelo:\n",
    "1. **Modelo de árbol de decisiones**\n",
    "    - Exactitud con el conjunto de datos de entrenamiento: 0.81410\n",
    "    - Exactitud con el conjunto de datos de prueba: 0.77736\n",
    "2. **Modelo de bosque aleatorio**\n",
    "    - Exactitud con el conjunto de datos de entrenamiento: 0.85518\n",
    "    - Exactitud con el conjunto de datos de prueba: 0.77860\n",
    "3. **Modelo de regresión logística**\n",
    "    - Exactitud con el conjunto de datos de entrenamiento: 0.69834\n",
    "    - Exactitud con el conjunto de datos de prueba: 0.69029\n",
    "  \n",
    "Cabe destacar que estamos eligiendo los mejores valores de cada modelo. En resumen, en números netos el modelo con mayor exactitud es el de bosque aleatorio (y con el tiempo que tomó, más le vale). Podemos avanzar tranquilos ya que pasamos el umbral por 0.0286 que no es mucho, pero es trabajo honesto.\n",
    "  \n",
    "Ya lo destaqué antes, pero a pesar de que elijo el modelo de bosque aleaetorio lo hago conscientemente de que es un modelo con un grado no menor de sobreajuste por lo que ponerlo a prueba con datos nuevos puede devolver una exactitud menor a la que vemos en las pruebas. Con esa aclaración lista, sigamos.\n",
    "  \n",
    "Con el modelo determinado, vamos a guardar el modelo en una variable aparte para ordenar bien todo y realizar la prueba de cordura."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_model = forest_model_2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora a explicar como haremos la prueba de cordura. Si bien podriamos crear un modelo que simplememte asigne 0 y 1 con la misma probabilidad, decir que tiene una exactitud menor que nuestro modelo y quedarnos con eso... Eso no sería algo que me deje conforme por lo que vamos a observar la distribucion de la columna `is_ultra` (nuestro target) en todo el df y vamos a comparar con la distribución de la predicción de nuestro modelo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Primero veamos la distribución del df completo\n",
    "pd.concat(\n",
    "    [df['is_ultra'].value_counts(normalize= True),\n",
    "    df['is_ultra'].value_counts()], axis=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ahora veamos como se comporta nuestro modelo\n",
    "pd.concat(\n",
    "[pd.Series(final_model.predict(df.drop(['is_ultra'],axis=1))).value_counts(normalize= True),\n",
    "pd.Series(final_model.predict(df.drop(['is_ultra'],axis=1))).value_counts()], axis= 1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_score(df['is_ultra'],final_model.predict(df.drop(['is_ultra'],axis=1)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Al ver unicamente las distribuciones (y principalmente las normalizadas) sentí que el modelo tuvo resultados peores de los esperados, pero al ver los numeros netos y el *accuracy_score* me volvió la esperanza al cuerpo. Al parecer nuestro modelo tiene muchos falsos negativos y al ver el accuarcy score se ve como éste se encuentra un poco por debajo del obtenido al predecir los datos de entrenamiento. Por supuesto, ésto tiene sentido ya que la mayoria de los datos del df son los de entrenamiento.\n",
    "  \n",
    "Entonces, con una diferencia normalizada de 0.11 podemos tener un buen grado de certeza de que nuestro modelo si pasa una prueba de cordura y es mejor que simplemente asignar aleatoriamente los resultados."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusión\n",
    "  \n",
    "Trabajar con éste proyecto fue ciertamente una prueba de... paciencia. Lo primero que tenemos que destcar (y agradecer) es la limpieza de los datos que nos tocaron ya que no tuvimos que realizar ningun tipo de intervención y pudimos pasar directamente al trabajo de verdad.\n",
    "  \n",
    "Originalmente planeaba trabajar con 3 conjuntos de datos: uno de entrenamiento, uno de validación y uno de prueba. El segundo iba a estar dedicado a ayudarnos a refinar los hiperparámetros de los modelos de clasificación para así posteriormente evaluar los modelos en sí con el conjunto de pruebas. Como vimos, eso no fue lo que hice ya que no mucho después de mi investigación para la refinación de los hiperparámetros me encontré con la función *GridSearchCV*. Ésta función logra de manera más eficiente y con mayor precisión el trabajo que mis nidos de bucles iban a lograr, y encima de eso también hace verificaciones cruzadas entre los datos. Gracias a ella, pude directamente dedicar el 75% de los datos al entrenamiento y el otro 25% a prueba. No tengo dudas de que eso ayudó a lograr un modelo de mejor calidad y posiblemente en un menor tiempo.\n",
    "  \n",
    "A la hora de trabajar con los modelos optamos por probar 3 modelos diferentes:\n",
    "1. **Árbol de decisiones**: Un modelo simple y rápido que va separando en base a las características para finalmente culminar en la clasificación a tomar (1 o 0).\n",
    "2. **Bosque aleatorio**: Un modelo que parte con el árbol de decisiones pero que expande la idea inicial creando múltiples árboles diferentes \"especializados\" en diferentes grupos de categorías y posteriormente toma una decisión ponderando los diferentes resultados. Como se ha de esperar, es un modelo con una alta carga computacional y un rendimiento logarítmico.\n",
    "3. **Regresión logística**: Nacido en 1958 a partir de la regresión lineal pero aplicado sobre un eje logarítmico en función de p/1-p lo que le permite obtener matemáticamente resultados categóricos a partir de valores continuos o discretos. Debido a su naturaleza matemática es muy eficiente computacionalmente pero con una capacidad de predicción que muchas veces queda corto.\n",
    "  \n",
    "Al trabajar con los modelos nos enteramos que nuestros datos presentaban unas características muy peculiares que se ven al entrar al detalle de los dos modelos de árbol que hicimos. Al ver de manera gráfica los árboles pudimos notar como éstos no logran refinar de manera uniforme los datos y éstos terminan con multiples hojas con una enorme cantidad de valores (una hoja tenia +1700 valores). Esa experiencia demostró que quizas el árbol de decisiones no era el mejor modelo para nuestros datos aunque si era suficiente ya que cumplía con el umbral establecido de 0,75.\n",
    "  \n",
    "Posteriormente trabajamos con el modelo de bosque aleatorio que demostró sin lugar a dudas que éste era el modelo más pesado computacionalmente pero que finalmente logró los mejores resultados alcanzando una exactitud con el conjunto de pruebas de un 0,7786. Cabe destacar que éste modelo presentaba el mayor sobreajuste de todos pero como nos preocupamos por la exactitud final nos quedamos con él.\n",
    "  \n",
    "Finalmente le dimos una prueba al modelo de regresión logística el cual se ejecutó bastante rápido y me obligó a leer muy detalladamente la documentación por unos errores que no llegaron al proyecto final. Como era de esperar el mismo presentó la menor exactitúd de los tres pero a su vez demostró el menor sobreajuste de todos. Éste no logró superar el umbral de 0,75.\n",
    "  \n",
    "En conclusión, logramos obtener un modelo que supere el umbral establecido por 0.0286 y que al ponerlo a someterlo a una prueba de cordura éste logró superarla aunque demostrando una tendencia a los falsos negativos. Y que se puede mejorar? Para empezar, nunca vienen mal más filas en nuestros datos, pero eso muchas veces no es posible, por lo que se podría buscar obtener nuevas columnas como si un cliente sobrepasó el límite mensual de datos/mensajes/llamadas o el monto total que cierto cliente pagó. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
