{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a id='toc1_'></a>[Salvando clientes de Beta Bank](#toc0_)\n",
    "  \n",
    "Los clientes de Beta Bank se están yendo, cada mes, poco a poco. Los banqueros descubrieron que es más barato salvar a los clientes existentes que atraer nuevos.\n",
    "  \n",
    "Necesitamos predecir si un cliente dejará el banco pronto. Tú tienes los datos sobre el comportamiento pasado de los clientes y la terminación de contratos con el banco.\n",
    "  \n",
    "Determinaremos que tan buen modelo tenemos en las manos utilizando métricicas como el puntaje F1 y el AUC-ROC."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Table of contents**<a id='toc0_'></a>    \n",
    "- [Salvando clientes de Beta Bank](#toc1_)    \n",
    "  - [Inicialización](#toc1_1_)    \n",
    "  - [Carga de datos](#toc1_2_)    \n",
    "  - [Exploración y carga inicial de datos](#toc1_3_)    \n",
    "    - [Análisis y reemplazo de valores ausentes en `tenure`](#toc1_3_1_)    \n",
    "    - [Preparacion de datos para machine learning](#toc1_3_2_)    \n",
    "  - [Evaluación de balance de clases](#toc1_4_)    \n",
    "    - [Sobremuestreo](#toc1_4_1_)    \n",
    "    - [Submuestreo](#toc1_4_2_)    \n",
    "  - [Segmentación de datos](#toc1_5_)    \n",
    "  - [Prueba de modelos e hiperparámetros](#toc1_6_)    \n",
    "    - [Modelo de bosque aleatorio](#toc1_6_1_)    \n",
    "      - [Conjunto de datos - Sobremuestreo](#toc1_6_1_1_)    \n",
    "      - [Conjunto de datos - Submuestreo](#toc1_6_1_2_)    \n",
    "      - [Conjunto de datos - Desbalance de clases](#toc1_6_1_3_)    \n",
    "    - [Modelo de regresión logística](#toc1_6_2_)    \n",
    "      - [Conjunto de datos - Sobremuestreo](#toc1_6_2_1_)    \n",
    "      - [Conjunto de datos - Submuestreo](#toc1_6_2_2_)    \n",
    "      - [Conjunto de datos - Desbalance](#toc1_6_2_3_)    \n",
    "  - [Prueba final](#toc1_7_)    \n",
    "  - [Conclusión](#toc1_8_)    \n",
    "\n",
    "<!-- vscode-jupyter-toc-config\n",
    "\tnumbering=false\n",
    "\tanchor=true\n",
    "\tflat=false\n",
    "\tminLevel=1\n",
    "\tmaxLevel=6\n",
    "\t/vscode-jupyter-toc-config -->\n",
    "<!-- THIS CELL WILL BE REPLACED ON TOC UPDATE. DO NOT WRITE YOUR TEXT IN THIS CELL -->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='toc1_1_'></a>[Inicialización](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import roc_auc_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='toc1_2_'></a>[Carga de datos](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('../datasets/Churn.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='toc1_3_'></a>[Exploración y carga inicial de datos](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 10000 entries, 0 to 9999\n",
      "Data columns (total 14 columns):\n",
      " #   Column           Non-Null Count  Dtype  \n",
      "---  ------           --------------  -----  \n",
      " 0   RowNumber        10000 non-null  int64  \n",
      " 1   CustomerId       10000 non-null  int64  \n",
      " 2   Surname          10000 non-null  object \n",
      " 3   CreditScore      10000 non-null  int64  \n",
      " 4   Geography        10000 non-null  object \n",
      " 5   Gender           10000 non-null  object \n",
      " 6   Age              10000 non-null  int64  \n",
      " 7   Tenure           9091 non-null   float64\n",
      " 8   Balance          10000 non-null  float64\n",
      " 9   NumOfProducts    10000 non-null  int64  \n",
      " 10  HasCrCard        10000 non-null  int64  \n",
      " 11  IsActiveMember   10000 non-null  int64  \n",
      " 12  EstimatedSalary  10000 non-null  float64\n",
      " 13  Exited           10000 non-null  int64  \n",
      "dtypes: float64(3), int64(8), object(3)\n",
      "memory usage: 1.1+ MB\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>RowNumber</th>\n",
       "      <th>CustomerId</th>\n",
       "      <th>Surname</th>\n",
       "      <th>CreditScore</th>\n",
       "      <th>Geography</th>\n",
       "      <th>Gender</th>\n",
       "      <th>Age</th>\n",
       "      <th>Tenure</th>\n",
       "      <th>Balance</th>\n",
       "      <th>NumOfProducts</th>\n",
       "      <th>HasCrCard</th>\n",
       "      <th>IsActiveMember</th>\n",
       "      <th>EstimatedSalary</th>\n",
       "      <th>Exited</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>15634602</td>\n",
       "      <td>Hargrave</td>\n",
       "      <td>619</td>\n",
       "      <td>France</td>\n",
       "      <td>Female</td>\n",
       "      <td>42</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>101348.88</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>15647311</td>\n",
       "      <td>Hill</td>\n",
       "      <td>608</td>\n",
       "      <td>Spain</td>\n",
       "      <td>Female</td>\n",
       "      <td>41</td>\n",
       "      <td>1.0</td>\n",
       "      <td>83807.86</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>112542.58</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>15619304</td>\n",
       "      <td>Onio</td>\n",
       "      <td>502</td>\n",
       "      <td>France</td>\n",
       "      <td>Female</td>\n",
       "      <td>42</td>\n",
       "      <td>8.0</td>\n",
       "      <td>159660.80</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>113931.57</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>15701354</td>\n",
       "      <td>Boni</td>\n",
       "      <td>699</td>\n",
       "      <td>France</td>\n",
       "      <td>Female</td>\n",
       "      <td>39</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>93826.63</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>15737888</td>\n",
       "      <td>Mitchell</td>\n",
       "      <td>850</td>\n",
       "      <td>Spain</td>\n",
       "      <td>Female</td>\n",
       "      <td>43</td>\n",
       "      <td>2.0</td>\n",
       "      <td>125510.82</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>79084.10</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9995</th>\n",
       "      <td>9996</td>\n",
       "      <td>15606229</td>\n",
       "      <td>Obijiaku</td>\n",
       "      <td>771</td>\n",
       "      <td>France</td>\n",
       "      <td>Male</td>\n",
       "      <td>39</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>96270.64</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9996</th>\n",
       "      <td>9997</td>\n",
       "      <td>15569892</td>\n",
       "      <td>Johnstone</td>\n",
       "      <td>516</td>\n",
       "      <td>France</td>\n",
       "      <td>Male</td>\n",
       "      <td>35</td>\n",
       "      <td>10.0</td>\n",
       "      <td>57369.61</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>101699.77</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9997</th>\n",
       "      <td>9998</td>\n",
       "      <td>15584532</td>\n",
       "      <td>Liu</td>\n",
       "      <td>709</td>\n",
       "      <td>France</td>\n",
       "      <td>Female</td>\n",
       "      <td>36</td>\n",
       "      <td>7.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>42085.58</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9998</th>\n",
       "      <td>9999</td>\n",
       "      <td>15682355</td>\n",
       "      <td>Sabbatini</td>\n",
       "      <td>772</td>\n",
       "      <td>Germany</td>\n",
       "      <td>Male</td>\n",
       "      <td>42</td>\n",
       "      <td>3.0</td>\n",
       "      <td>75075.31</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>92888.52</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9999</th>\n",
       "      <td>10000</td>\n",
       "      <td>15628319</td>\n",
       "      <td>Walker</td>\n",
       "      <td>792</td>\n",
       "      <td>France</td>\n",
       "      <td>Female</td>\n",
       "      <td>28</td>\n",
       "      <td>NaN</td>\n",
       "      <td>130142.79</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>38190.78</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10000 rows × 14 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      RowNumber  CustomerId    Surname  CreditScore Geography  Gender  Age  \\\n",
       "0             1    15634602   Hargrave          619    France  Female   42   \n",
       "1             2    15647311       Hill          608     Spain  Female   41   \n",
       "2             3    15619304       Onio          502    France  Female   42   \n",
       "3             4    15701354       Boni          699    France  Female   39   \n",
       "4             5    15737888   Mitchell          850     Spain  Female   43   \n",
       "...         ...         ...        ...          ...       ...     ...  ...   \n",
       "9995       9996    15606229   Obijiaku          771    France    Male   39   \n",
       "9996       9997    15569892  Johnstone          516    France    Male   35   \n",
       "9997       9998    15584532        Liu          709    France  Female   36   \n",
       "9998       9999    15682355  Sabbatini          772   Germany    Male   42   \n",
       "9999      10000    15628319     Walker          792    France  Female   28   \n",
       "\n",
       "      Tenure    Balance  NumOfProducts  HasCrCard  IsActiveMember  \\\n",
       "0        2.0       0.00              1          1               1   \n",
       "1        1.0   83807.86              1          0               1   \n",
       "2        8.0  159660.80              3          1               0   \n",
       "3        1.0       0.00              2          0               0   \n",
       "4        2.0  125510.82              1          1               1   \n",
       "...      ...        ...            ...        ...             ...   \n",
       "9995     5.0       0.00              2          1               0   \n",
       "9996    10.0   57369.61              1          1               1   \n",
       "9997     7.0       0.00              1          0               1   \n",
       "9998     3.0   75075.31              2          1               0   \n",
       "9999     NaN  130142.79              1          1               0   \n",
       "\n",
       "      EstimatedSalary  Exited  \n",
       "0           101348.88       1  \n",
       "1           112542.58       0  \n",
       "2           113931.57       1  \n",
       "3            93826.63       0  \n",
       "4            79084.10       0  \n",
       "...               ...     ...  \n",
       "9995         96270.64       0  \n",
       "9996        101699.77       0  \n",
       "9997         42085.58       1  \n",
       "9998         92888.52       1  \n",
       "9999         38190.78       0  \n",
       "\n",
       "[10000 rows x 14 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Primero vamos a ver que nos dice info y un panorama de como se ve el DataFrame\n",
    "data.info()\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "De lo que podemos observar, las columnas que tenemos son:\n",
    "- **RowNumber**: índice de cadena de datos\n",
    "- **CustomerId**: identificador de cliente único\n",
    "- **Surname**: apellido\n",
    "- **CreditScore**: valor de crédito\n",
    "- **Geography**: país de residencia\n",
    "- **Gender**: sexo\n",
    "- **Age**: edad\n",
    "- **Tenure**: período durante el cual ha madurado el depósito a plazo fijo de un cliente (años)\n",
    "- **Balance**: saldo de la cuenta\n",
    "- **NumOfProducts**: número de productos bancarios utilizados por el cliente\n",
    "- **HasCrCard**: el cliente tiene una tarjeta de crédito (1 - sí; 0 - no)\n",
    "- **IsActiveMember**: actividad del cliente (1 - sí; 0 - no)\n",
    "- **EstimatedSalary**: salario estimado\n",
    "  \n",
    "<ins>**Objetivo**</ins>\n",
    "- **Exited**: El cliente se ha ido (1 - sí; 0 - no)\n",
    "  \n",
    "Lo primero que noto de la tabla es que vamos a tener que renombrar las columnas, y para mi tristeza tendrá que ser manualmente. De ahí podemos observar que la única columna con valores nulos es `Tenure` por la que tendremos que indagar un poco más adelante.\n",
    "  \n",
    "Ahora, con el modelo en mente, puedo observar como hay 3 columnas que no nos van a ser muy útiles para entrenar nuestro modelo. Esas son:\n",
    "1. RowNumber\n",
    "2. CustomerID\n",
    "3. Surname\n",
    "  \n",
    "Eso se debe a que esas columnas son únicas para cada cliente y no podremos obtener ninguna conclusión con los valores obtenidos de esas celdas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Antes de seguir mucho más, quiero corregir las columnas\n",
    "data = data.rename(\n",
    "    columns={\n",
    "        'RowNumber':'row_number',\n",
    "        'CustomerId':'customer_id',\n",
    "        'Surname':'surname',\n",
    "        'CreditScore':'credit_score',\n",
    "        'Geography':'geography',\n",
    "        'Gender':'gender',\n",
    "        'Age':'age',\n",
    "        'Tenure':'tenure',\n",
    "        'Balance':'balance',\n",
    "        'NumOfProducts':'num_of_products',\n",
    "        'HasCrCard':'has_cr_card',\n",
    "        'IsActiveMember':'is_active',\n",
    "        'EstimatedSalary':'estimated_salary',\n",
    "        'Exited':'exited'\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tuve que hacerlo manualmente ya que algunas columnas se les tenia que agregar los _ \n",
    "  \n",
    "Podría haber modificado manualmente los que necesitaban _ y aplicar lower al resto? Si, pero se me acaba de ocurrir esa idea."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Antes de seguir observemos si hay duplicados en nuestra tabla\n",
    "data.duplicated().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>row_number</th>\n",
       "      <th>customer_id</th>\n",
       "      <th>credit_score</th>\n",
       "      <th>age</th>\n",
       "      <th>tenure</th>\n",
       "      <th>balance</th>\n",
       "      <th>num_of_products</th>\n",
       "      <th>has_cr_card</th>\n",
       "      <th>is_active</th>\n",
       "      <th>estimated_salary</th>\n",
       "      <th>exited</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>10000.00000</td>\n",
       "      <td>1.000000e+04</td>\n",
       "      <td>10000.000000</td>\n",
       "      <td>10000.000000</td>\n",
       "      <td>9091.000000</td>\n",
       "      <td>10000.000000</td>\n",
       "      <td>10000.000000</td>\n",
       "      <td>10000.00000</td>\n",
       "      <td>10000.000000</td>\n",
       "      <td>10000.000000</td>\n",
       "      <td>10000.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>5000.50000</td>\n",
       "      <td>1.569094e+07</td>\n",
       "      <td>650.528800</td>\n",
       "      <td>38.921800</td>\n",
       "      <td>4.997690</td>\n",
       "      <td>76485.889288</td>\n",
       "      <td>1.530200</td>\n",
       "      <td>0.70550</td>\n",
       "      <td>0.515100</td>\n",
       "      <td>100090.239881</td>\n",
       "      <td>0.203700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>2886.89568</td>\n",
       "      <td>7.193619e+04</td>\n",
       "      <td>96.653299</td>\n",
       "      <td>10.487806</td>\n",
       "      <td>2.894723</td>\n",
       "      <td>62397.405202</td>\n",
       "      <td>0.581654</td>\n",
       "      <td>0.45584</td>\n",
       "      <td>0.499797</td>\n",
       "      <td>57510.492818</td>\n",
       "      <td>0.402769</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>1.00000</td>\n",
       "      <td>1.556570e+07</td>\n",
       "      <td>350.000000</td>\n",
       "      <td>18.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>11.580000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>2500.75000</td>\n",
       "      <td>1.562853e+07</td>\n",
       "      <td>584.000000</td>\n",
       "      <td>32.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>51002.110000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>5000.50000</td>\n",
       "      <td>1.569074e+07</td>\n",
       "      <td>652.000000</td>\n",
       "      <td>37.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>97198.540000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>100193.915000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>7500.25000</td>\n",
       "      <td>1.575323e+07</td>\n",
       "      <td>718.000000</td>\n",
       "      <td>44.000000</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>127644.240000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>149388.247500</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>10000.00000</td>\n",
       "      <td>1.581569e+07</td>\n",
       "      <td>850.000000</td>\n",
       "      <td>92.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>250898.090000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>199992.480000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        row_number   customer_id  credit_score           age       tenure  \\\n",
       "count  10000.00000  1.000000e+04  10000.000000  10000.000000  9091.000000   \n",
       "mean    5000.50000  1.569094e+07    650.528800     38.921800     4.997690   \n",
       "std     2886.89568  7.193619e+04     96.653299     10.487806     2.894723   \n",
       "min        1.00000  1.556570e+07    350.000000     18.000000     0.000000   \n",
       "25%     2500.75000  1.562853e+07    584.000000     32.000000     2.000000   \n",
       "50%     5000.50000  1.569074e+07    652.000000     37.000000     5.000000   \n",
       "75%     7500.25000  1.575323e+07    718.000000     44.000000     7.000000   \n",
       "max    10000.00000  1.581569e+07    850.000000     92.000000    10.000000   \n",
       "\n",
       "             balance  num_of_products  has_cr_card     is_active  \\\n",
       "count   10000.000000     10000.000000  10000.00000  10000.000000   \n",
       "mean    76485.889288         1.530200      0.70550      0.515100   \n",
       "std     62397.405202         0.581654      0.45584      0.499797   \n",
       "min         0.000000         1.000000      0.00000      0.000000   \n",
       "25%         0.000000         1.000000      0.00000      0.000000   \n",
       "50%     97198.540000         1.000000      1.00000      1.000000   \n",
       "75%    127644.240000         2.000000      1.00000      1.000000   \n",
       "max    250898.090000         4.000000      1.00000      1.000000   \n",
       "\n",
       "       estimated_salary        exited  \n",
       "count      10000.000000  10000.000000  \n",
       "mean      100090.239881      0.203700  \n",
       "std        57510.492818      0.402769  \n",
       "min           11.580000      0.000000  \n",
       "25%        51002.110000      0.000000  \n",
       "50%       100193.915000      0.000000  \n",
       "75%       149388.247500      0.000000  \n",
       "max       199992.480000      1.000000  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Tambien veamos que nos dice el describe de la tabla\n",
    "data.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Quise hacer ésto ya que nos permite evaluar la distribución de los datos numéricos de manera más rápida. Tras un par de cálculos mentales podemos notar que la única columna que presenta outliers (valores alejados más de 3 desviaciones estándar de la medaiana) es `num_of_products` que... podemos permitirle ya que por más que se vaya de lo normal, es un factor importante a tener para el banco.\n",
    "  \n",
    "Pero a fin de cuentas, éste describe me da tranquilidad ya que no voy a tener que eliminar valores outliers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    7963\n",
       "1    2037\n",
       "Name: exited, dtype: int64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Ahora observemos bien como se distribuyen los datos\n",
    "data['exited'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El hecho de que sean 10000 filas nos permite saber los porcentajes fácilmente y ,aunque desconozco precisamente el margen de tiempo que cubren los datos, que se vayan wl 20% de los clientes ciertamente no es una buena señal. Veamos un poco las columnas `geography`, `num_of_products` y `estimated_salary` en contraste con los exited y no."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Primero separemos los que se fueron en una variable para compararlos con todo el df\n",
    "exited = data.loc[data.exited == 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>geography</th>\n",
       "      <th>geography</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Germany</th>\n",
       "      <td>0.399607</td>\n",
       "      <td>0.2509</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>France</th>\n",
       "      <td>0.397644</td>\n",
       "      <td>0.5014</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Spain</th>\n",
       "      <td>0.202749</td>\n",
       "      <td>0.2477</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         geography  geography\n",
       "Germany   0.399607     0.2509\n",
       "France    0.397644     0.5014\n",
       "Spain     0.202749     0.2477"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Ahora veamos rápido la comparativa\n",
    "pd.concat(\n",
    "    [exited['geography'].value_counts(normalize=True),\n",
    "     data['geography'].value_counts(normalize=True)],\n",
    "    axis= 1\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A primera vista notamos como los clientes alemanes parecen más propensos a dejar el banco, mientras que Francia y España no se encuentran muy diferentes entre sí. Acaso los beneficios que da el banco no son los mejores en el país de las cervezas?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>num_of_products</th>\n",
       "      <th>num_of_products</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.691703</td>\n",
       "      <td>0.5084</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.170839</td>\n",
       "      <td>0.4590</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.108002</td>\n",
       "      <td>0.0266</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.029455</td>\n",
       "      <td>0.0060</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   num_of_products  num_of_products\n",
       "1         0.691703           0.5084\n",
       "2         0.170839           0.4590\n",
       "3         0.108002           0.0266\n",
       "4         0.029455           0.0060"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Hay algún detalle similar en num_of_products?\n",
    "pd.concat(\n",
    "    [\n",
    "        exited['num_of_products'].value_counts(normalize=True),\n",
    "         data['num_of_products'].value_counts(normalize=True)\n",
    "    ],\n",
    "    axis= 1\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A pesar de que la mayoria de los clientes tiene entre 1 y 2 productos del banco, el grueso de los que se van del banco tenían 1 producto, capaz que si se les ofrece un beneficio inicial para algún otro producto se logre un gran paso para que se queden!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>estimated_salary</th>\n",
       "      <th>estimated_salary</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>2037.000000</td>\n",
       "      <td>10000.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>101465.677531</td>\n",
       "      <td>100090.239881</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>57912.418071</td>\n",
       "      <td>57510.492818</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>11.580000</td>\n",
       "      <td>11.580000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>51907.720000</td>\n",
       "      <td>51002.110000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>102460.840000</td>\n",
       "      <td>100193.915000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>152422.910000</td>\n",
       "      <td>149388.247500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>199808.100000</td>\n",
       "      <td>199992.480000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       estimated_salary  estimated_salary\n",
       "count       2037.000000      10000.000000\n",
       "mean      101465.677531     100090.239881\n",
       "std        57912.418071      57510.492818\n",
       "min           11.580000         11.580000\n",
       "25%        51907.720000      51002.110000\n",
       "50%       102460.840000     100193.915000\n",
       "75%       152422.910000     149388.247500\n",
       "max       199808.100000     199992.480000"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Finalmente veamos estimated_salary\n",
    "pd.concat(\n",
    "    [\n",
    "        exited['estimated_salary'].describe(),\n",
    "        data['estimated_salary'].describe()\n",
    "    ],\n",
    "    axis= 1\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Acá vemos que tienen patrones muy similares entre sí por lo que podemos confiar en que el sueldo de una persona no influencia en si se va a ir o no del banco, lo que es una buena señal!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Por lo tanto, a primera vista podemos concluir que efectivamente la situación del banco es una que merece ser destacada y tratada con urgencia ya que 2000 clientes no es poca cosa. Los factores que llevan a que un cliente se vaya parecen depender en parte de su país de residencia y de la cantidad de productos que tengan pero no de cuanto ganan. Se podrían estudiar todas las columnas y realizar un análisis mucho más profundo al respecto pero no es el enfoque que vamos a tratar hoy. Hoy vamos a buscar hacer el modelo perfecto!... O acercarnos lo suficiente."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='toc1_3_1_'></a>[Análisis y reemplazo de valores ausentes en `tenure`](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Primero guardemos en una variable las filas que les falta tenure y en otra las que no\n",
    "missing_tenure = data.loc[data.tenure.isna()]\n",
    "\n",
    "not_missing_tenure = data.loc[~data.tenure.isna()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>row_number</th>\n",
       "      <th>customer_id</th>\n",
       "      <th>surname</th>\n",
       "      <th>credit_score</th>\n",
       "      <th>geography</th>\n",
       "      <th>gender</th>\n",
       "      <th>age</th>\n",
       "      <th>tenure</th>\n",
       "      <th>balance</th>\n",
       "      <th>num_of_products</th>\n",
       "      <th>has_cr_card</th>\n",
       "      <th>is_active</th>\n",
       "      <th>estimated_salary</th>\n",
       "      <th>exited</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>31</td>\n",
       "      <td>15589475</td>\n",
       "      <td>Azikiwe</td>\n",
       "      <td>591</td>\n",
       "      <td>Spain</td>\n",
       "      <td>Female</td>\n",
       "      <td>39</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.00</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>140469.38</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>49</td>\n",
       "      <td>15766205</td>\n",
       "      <td>Yin</td>\n",
       "      <td>550</td>\n",
       "      <td>Germany</td>\n",
       "      <td>Male</td>\n",
       "      <td>38</td>\n",
       "      <td>NaN</td>\n",
       "      <td>103391.38</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>90878.13</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>52</td>\n",
       "      <td>15768193</td>\n",
       "      <td>Trevisani</td>\n",
       "      <td>585</td>\n",
       "      <td>Germany</td>\n",
       "      <td>Male</td>\n",
       "      <td>36</td>\n",
       "      <td>NaN</td>\n",
       "      <td>146050.97</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>86424.57</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53</th>\n",
       "      <td>54</td>\n",
       "      <td>15702298</td>\n",
       "      <td>Parkhill</td>\n",
       "      <td>655</td>\n",
       "      <td>Germany</td>\n",
       "      <td>Male</td>\n",
       "      <td>41</td>\n",
       "      <td>NaN</td>\n",
       "      <td>125561.97</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>164040.94</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60</th>\n",
       "      <td>61</td>\n",
       "      <td>15651280</td>\n",
       "      <td>Hunter</td>\n",
       "      <td>742</td>\n",
       "      <td>Germany</td>\n",
       "      <td>Male</td>\n",
       "      <td>35</td>\n",
       "      <td>NaN</td>\n",
       "      <td>136857.00</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>84509.57</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9944</th>\n",
       "      <td>9945</td>\n",
       "      <td>15703923</td>\n",
       "      <td>Cameron</td>\n",
       "      <td>744</td>\n",
       "      <td>Germany</td>\n",
       "      <td>Male</td>\n",
       "      <td>41</td>\n",
       "      <td>NaN</td>\n",
       "      <td>190409.34</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>138361.48</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9956</th>\n",
       "      <td>9957</td>\n",
       "      <td>15707861</td>\n",
       "      <td>Nucci</td>\n",
       "      <td>520</td>\n",
       "      <td>France</td>\n",
       "      <td>Female</td>\n",
       "      <td>46</td>\n",
       "      <td>NaN</td>\n",
       "      <td>85216.61</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>117369.52</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9964</th>\n",
       "      <td>9965</td>\n",
       "      <td>15642785</td>\n",
       "      <td>Douglas</td>\n",
       "      <td>479</td>\n",
       "      <td>France</td>\n",
       "      <td>Male</td>\n",
       "      <td>34</td>\n",
       "      <td>NaN</td>\n",
       "      <td>117593.48</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>113308.29</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9985</th>\n",
       "      <td>9986</td>\n",
       "      <td>15586914</td>\n",
       "      <td>Nepean</td>\n",
       "      <td>659</td>\n",
       "      <td>France</td>\n",
       "      <td>Male</td>\n",
       "      <td>36</td>\n",
       "      <td>NaN</td>\n",
       "      <td>123841.49</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>96833.00</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9999</th>\n",
       "      <td>10000</td>\n",
       "      <td>15628319</td>\n",
       "      <td>Walker</td>\n",
       "      <td>792</td>\n",
       "      <td>France</td>\n",
       "      <td>Female</td>\n",
       "      <td>28</td>\n",
       "      <td>NaN</td>\n",
       "      <td>130142.79</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>38190.78</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>909 rows × 14 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      row_number  customer_id    surname  credit_score geography  gender  age  \\\n",
       "30            31     15589475    Azikiwe           591     Spain  Female   39   \n",
       "48            49     15766205        Yin           550   Germany    Male   38   \n",
       "51            52     15768193  Trevisani           585   Germany    Male   36   \n",
       "53            54     15702298   Parkhill           655   Germany    Male   41   \n",
       "60            61     15651280     Hunter           742   Germany    Male   35   \n",
       "...          ...          ...        ...           ...       ...     ...  ...   \n",
       "9944        9945     15703923    Cameron           744   Germany    Male   41   \n",
       "9956        9957     15707861      Nucci           520    France  Female   46   \n",
       "9964        9965     15642785    Douglas           479    France    Male   34   \n",
       "9985        9986     15586914     Nepean           659    France    Male   36   \n",
       "9999       10000     15628319     Walker           792    France  Female   28   \n",
       "\n",
       "      tenure    balance  num_of_products  has_cr_card  is_active  \\\n",
       "30       NaN       0.00                3            1          0   \n",
       "48       NaN  103391.38                1            0          1   \n",
       "51       NaN  146050.97                2            0          0   \n",
       "53       NaN  125561.97                1            0          0   \n",
       "60       NaN  136857.00                1            0          0   \n",
       "...      ...        ...              ...          ...        ...   \n",
       "9944     NaN  190409.34                2            1          1   \n",
       "9956     NaN   85216.61                1            1          0   \n",
       "9964     NaN  117593.48                2            0          0   \n",
       "9985     NaN  123841.49                2            1          0   \n",
       "9999     NaN  130142.79                1            1          0   \n",
       "\n",
       "      estimated_salary  exited  \n",
       "30           140469.38       1  \n",
       "48            90878.13       0  \n",
       "51            86424.57       0  \n",
       "53           164040.94       1  \n",
       "60            84509.57       0  \n",
       "...                ...     ...  \n",
       "9944         138361.48       0  \n",
       "9956         117369.52       1  \n",
       "9964         113308.29       0  \n",
       "9985          96833.00       0  \n",
       "9999          38190.78       0  \n",
       "\n",
       "[909 rows x 14 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Veamos entonces las filas con ausentes\n",
    "missing_tenure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>row_number</th>\n",
       "      <th>customer_id</th>\n",
       "      <th>surname</th>\n",
       "      <th>credit_score</th>\n",
       "      <th>geography</th>\n",
       "      <th>gender</th>\n",
       "      <th>age</th>\n",
       "      <th>tenure</th>\n",
       "      <th>balance</th>\n",
       "      <th>num_of_products</th>\n",
       "      <th>has_cr_card</th>\n",
       "      <th>is_active</th>\n",
       "      <th>estimated_salary</th>\n",
       "      <th>exited</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>15634602</td>\n",
       "      <td>Hargrave</td>\n",
       "      <td>619</td>\n",
       "      <td>France</td>\n",
       "      <td>Female</td>\n",
       "      <td>42</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>101348.88</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>15647311</td>\n",
       "      <td>Hill</td>\n",
       "      <td>608</td>\n",
       "      <td>Spain</td>\n",
       "      <td>Female</td>\n",
       "      <td>41</td>\n",
       "      <td>1.0</td>\n",
       "      <td>83807.86</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>112542.58</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>15619304</td>\n",
       "      <td>Onio</td>\n",
       "      <td>502</td>\n",
       "      <td>France</td>\n",
       "      <td>Female</td>\n",
       "      <td>42</td>\n",
       "      <td>8.0</td>\n",
       "      <td>159660.80</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>113931.57</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>15701354</td>\n",
       "      <td>Boni</td>\n",
       "      <td>699</td>\n",
       "      <td>France</td>\n",
       "      <td>Female</td>\n",
       "      <td>39</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>93826.63</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>15737888</td>\n",
       "      <td>Mitchell</td>\n",
       "      <td>850</td>\n",
       "      <td>Spain</td>\n",
       "      <td>Female</td>\n",
       "      <td>43</td>\n",
       "      <td>2.0</td>\n",
       "      <td>125510.82</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>79084.10</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9994</th>\n",
       "      <td>9995</td>\n",
       "      <td>15719294</td>\n",
       "      <td>Wood</td>\n",
       "      <td>800</td>\n",
       "      <td>France</td>\n",
       "      <td>Female</td>\n",
       "      <td>29</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>167773.55</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9995</th>\n",
       "      <td>9996</td>\n",
       "      <td>15606229</td>\n",
       "      <td>Obijiaku</td>\n",
       "      <td>771</td>\n",
       "      <td>France</td>\n",
       "      <td>Male</td>\n",
       "      <td>39</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>96270.64</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9996</th>\n",
       "      <td>9997</td>\n",
       "      <td>15569892</td>\n",
       "      <td>Johnstone</td>\n",
       "      <td>516</td>\n",
       "      <td>France</td>\n",
       "      <td>Male</td>\n",
       "      <td>35</td>\n",
       "      <td>10.0</td>\n",
       "      <td>57369.61</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>101699.77</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9997</th>\n",
       "      <td>9998</td>\n",
       "      <td>15584532</td>\n",
       "      <td>Liu</td>\n",
       "      <td>709</td>\n",
       "      <td>France</td>\n",
       "      <td>Female</td>\n",
       "      <td>36</td>\n",
       "      <td>7.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>42085.58</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9998</th>\n",
       "      <td>9999</td>\n",
       "      <td>15682355</td>\n",
       "      <td>Sabbatini</td>\n",
       "      <td>772</td>\n",
       "      <td>Germany</td>\n",
       "      <td>Male</td>\n",
       "      <td>42</td>\n",
       "      <td>3.0</td>\n",
       "      <td>75075.31</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>92888.52</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>9091 rows × 14 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      row_number  customer_id    surname  credit_score geography  gender  age  \\\n",
       "0              1     15634602   Hargrave           619    France  Female   42   \n",
       "1              2     15647311       Hill           608     Spain  Female   41   \n",
       "2              3     15619304       Onio           502    France  Female   42   \n",
       "3              4     15701354       Boni           699    France  Female   39   \n",
       "4              5     15737888   Mitchell           850     Spain  Female   43   \n",
       "...          ...          ...        ...           ...       ...     ...  ...   \n",
       "9994        9995     15719294       Wood           800    France  Female   29   \n",
       "9995        9996     15606229   Obijiaku           771    France    Male   39   \n",
       "9996        9997     15569892  Johnstone           516    France    Male   35   \n",
       "9997        9998     15584532        Liu           709    France  Female   36   \n",
       "9998        9999     15682355  Sabbatini           772   Germany    Male   42   \n",
       "\n",
       "      tenure    balance  num_of_products  has_cr_card  is_active  \\\n",
       "0        2.0       0.00                1            1          1   \n",
       "1        1.0   83807.86                1            0          1   \n",
       "2        8.0  159660.80                3            1          0   \n",
       "3        1.0       0.00                2            0          0   \n",
       "4        2.0  125510.82                1            1          1   \n",
       "...      ...        ...              ...          ...        ...   \n",
       "9994     2.0       0.00                2            0          0   \n",
       "9995     5.0       0.00                2            1          0   \n",
       "9996    10.0   57369.61                1            1          1   \n",
       "9997     7.0       0.00                1            0          1   \n",
       "9998     3.0   75075.31                2            1          0   \n",
       "\n",
       "      estimated_salary  exited  \n",
       "0            101348.88       1  \n",
       "1            112542.58       0  \n",
       "2            113931.57       1  \n",
       "3             93826.63       0  \n",
       "4             79084.10       0  \n",
       "...                ...     ...  \n",
       "9994         167773.55       0  \n",
       "9995          96270.64       0  \n",
       "9996         101699.77       0  \n",
       "9997          42085.58       1  \n",
       "9998          92888.52       1  \n",
       "\n",
       "[9091 rows x 14 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "not_missing_tenure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 0      1\n",
      "credit_score             \n",
      "350            NaN    5.0\n",
      "351            NaN    1.0\n",
      "358            NaN    1.0\n",
      "359            1.0    NaN\n",
      "363            NaN    1.0\n",
      "...            ...    ...\n",
      "846            1.0    4.0\n",
      "847            NaN    6.0\n",
      "848            NaN    5.0\n",
      "849            NaN    8.0\n",
      "850           23.0  210.0\n",
      "\n",
      "[460 rows x 2 columns] \n",
      "\n",
      "             0     1\n",
      "geography           \n",
      "France     464  4550\n",
      "Spain      229  2248\n",
      "Germany    216  2293 \n",
      "\n",
      "          0     1\n",
      "gender           \n",
      "Male    483  4974\n",
      "Female  426  4117 \n",
      "\n",
      "       0   1\n",
      "age         \n",
      "18   2.0  20\n",
      "19   1.0  26\n",
      "20   3.0  37\n",
      "21   5.0  48\n",
      "22   4.0  80\n",
      "..   ...  ..\n",
      "83   NaN   1\n",
      "84   NaN   2\n",
      "85   NaN   1\n",
      "88   NaN   1\n",
      "92   1.0   1\n",
      "\n",
      "[70 rows x 2 columns] \n",
      "\n",
      "               0       1\n",
      "balance                 \n",
      "0.00       334.0  3283.0\n",
      "3768.69      NaN     1.0\n",
      "12459.19     NaN     1.0\n",
      "14262.80     NaN     1.0\n",
      "16893.59     1.0     NaN\n",
      "...          ...     ...\n",
      "216109.88    NaN     1.0\n",
      "221532.80    NaN     1.0\n",
      "222267.63    NaN     1.0\n",
      "238387.56    NaN     1.0\n",
      "250898.09    NaN     1.0\n",
      "\n",
      "[6382 rows x 2 columns] \n",
      "\n",
      "                   0     1\n",
      "num_of_products           \n",
      "1                467  4617\n",
      "2                406  4184\n",
      "3                 32   234\n",
      "4                  4    56 \n",
      "\n",
      "               0     1\n",
      "has_cr_card           \n",
      "1            646  6409\n",
      "0            263  2682 \n",
      "\n",
      "             0     1\n",
      "is_active           \n",
      "1          464  4687\n",
      "0          445  4404 \n",
      "\n",
      "                    0    1\n",
      "estimated_salary          \n",
      "11.58             NaN  1.0\n",
      "90.07             NaN  1.0\n",
      "91.75             NaN  1.0\n",
      "96.27             NaN  1.0\n",
      "106.67            1.0  NaN\n",
      "...               ...  ...\n",
      "199909.32         NaN  1.0\n",
      "199929.17         NaN  1.0\n",
      "199953.33         NaN  1.0\n",
      "199970.74         NaN  1.0\n",
      "199992.48         NaN  1.0\n",
      "\n",
      "[9999 rows x 2 columns] \n",
      "\n",
      "          0     1\n",
      "exited           \n",
      "0       726  7237\n",
      "1       183  1854 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "for column in data.columns.values:\n",
    "    if column not in ['row_number','customer_id','surname','tenure']:\n",
    "        print(pd.concat(\n",
    "        [\n",
    "            missing_tenure.value_counts(column),\n",
    "            not_missing_tenure.value_counts(column)\n",
    "        ], axis=1\n",
    "        ),'\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para nuestra mala suerte, no hay un patrón muy evidente respecto a la causa de los ausentes por lo que vamos a tener que buscar la forma de imputar el valor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    9091.000000\n",
       "mean        4.997690\n",
       "std         2.894723\n",
       "min         0.000000\n",
       "25%         2.000000\n",
       "50%         5.000000\n",
       "75%         7.000000\n",
       "max        10.000000\n",
       "Name: tenure, dtype: float64"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Primero observemos la distribución de tenure\n",
    "not_missing_tenure['tenure'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Viendo que la mediana y la media son iguales, vamos a reemplazar los valores ausentes con 5\n",
    "filled_data = data.fillna(5)\n",
    "\n",
    "# Y tambien vamos a guardar una tabla aparte en donde solo vamos a tener valores puros, para comparar\n",
    "clean_data = not_missing_tenure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    9091.000000\n",
       "mean        4.997690\n",
       "std         2.894723\n",
       "min         0.000000\n",
       "25%         2.000000\n",
       "50%         5.000000\n",
       "75%         7.000000\n",
       "max        10.000000\n",
       "Name: tenure, dtype: float64"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Finalmente verificamos el impacto del cambio en nuestros datos\n",
    "data['tenure'].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dado que el cambio que hicimos no tuvo un impacto notorio, vamos a mantener éste cambio y proseguir con el proyecto. Posteriormente haremos comparaciones tambien para ver si es mejor el dataset limpio o con los valores imputados."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='toc1_3_2_'></a>[Preparacion de datos para machine learning](#toc0_)\n",
    "  \n",
    "Ahora nos dedicaremos a procesar los datos de tal forma que queden aptos para entrenar a un modelo. Eso incluirá principalmente transformar las columnas categóricas en multiples columnas OHE (One-Hot Encoding).\n",
    "  \n",
    "Para eso, primero observemos una vez más la tabla."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>row_number</th>\n",
       "      <th>customer_id</th>\n",
       "      <th>surname</th>\n",
       "      <th>credit_score</th>\n",
       "      <th>geography</th>\n",
       "      <th>gender</th>\n",
       "      <th>age</th>\n",
       "      <th>tenure</th>\n",
       "      <th>balance</th>\n",
       "      <th>num_of_products</th>\n",
       "      <th>has_cr_card</th>\n",
       "      <th>is_active</th>\n",
       "      <th>estimated_salary</th>\n",
       "      <th>exited</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>15634602</td>\n",
       "      <td>Hargrave</td>\n",
       "      <td>619</td>\n",
       "      <td>France</td>\n",
       "      <td>Female</td>\n",
       "      <td>42</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>101348.88</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>15647311</td>\n",
       "      <td>Hill</td>\n",
       "      <td>608</td>\n",
       "      <td>Spain</td>\n",
       "      <td>Female</td>\n",
       "      <td>41</td>\n",
       "      <td>1.0</td>\n",
       "      <td>83807.86</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>112542.58</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>15619304</td>\n",
       "      <td>Onio</td>\n",
       "      <td>502</td>\n",
       "      <td>France</td>\n",
       "      <td>Female</td>\n",
       "      <td>42</td>\n",
       "      <td>8.0</td>\n",
       "      <td>159660.80</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>113931.57</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>15701354</td>\n",
       "      <td>Boni</td>\n",
       "      <td>699</td>\n",
       "      <td>France</td>\n",
       "      <td>Female</td>\n",
       "      <td>39</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>93826.63</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>15737888</td>\n",
       "      <td>Mitchell</td>\n",
       "      <td>850</td>\n",
       "      <td>Spain</td>\n",
       "      <td>Female</td>\n",
       "      <td>43</td>\n",
       "      <td>2.0</td>\n",
       "      <td>125510.82</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>79084.10</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9995</th>\n",
       "      <td>9996</td>\n",
       "      <td>15606229</td>\n",
       "      <td>Obijiaku</td>\n",
       "      <td>771</td>\n",
       "      <td>France</td>\n",
       "      <td>Male</td>\n",
       "      <td>39</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>96270.64</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9996</th>\n",
       "      <td>9997</td>\n",
       "      <td>15569892</td>\n",
       "      <td>Johnstone</td>\n",
       "      <td>516</td>\n",
       "      <td>France</td>\n",
       "      <td>Male</td>\n",
       "      <td>35</td>\n",
       "      <td>10.0</td>\n",
       "      <td>57369.61</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>101699.77</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9997</th>\n",
       "      <td>9998</td>\n",
       "      <td>15584532</td>\n",
       "      <td>Liu</td>\n",
       "      <td>709</td>\n",
       "      <td>France</td>\n",
       "      <td>Female</td>\n",
       "      <td>36</td>\n",
       "      <td>7.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>42085.58</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9998</th>\n",
       "      <td>9999</td>\n",
       "      <td>15682355</td>\n",
       "      <td>Sabbatini</td>\n",
       "      <td>772</td>\n",
       "      <td>Germany</td>\n",
       "      <td>Male</td>\n",
       "      <td>42</td>\n",
       "      <td>3.0</td>\n",
       "      <td>75075.31</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>92888.52</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9999</th>\n",
       "      <td>10000</td>\n",
       "      <td>15628319</td>\n",
       "      <td>Walker</td>\n",
       "      <td>792</td>\n",
       "      <td>France</td>\n",
       "      <td>Female</td>\n",
       "      <td>28</td>\n",
       "      <td>NaN</td>\n",
       "      <td>130142.79</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>38190.78</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10000 rows × 14 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      row_number  customer_id    surname  credit_score geography  gender  age  \\\n",
       "0              1     15634602   Hargrave           619    France  Female   42   \n",
       "1              2     15647311       Hill           608     Spain  Female   41   \n",
       "2              3     15619304       Onio           502    France  Female   42   \n",
       "3              4     15701354       Boni           699    France  Female   39   \n",
       "4              5     15737888   Mitchell           850     Spain  Female   43   \n",
       "...          ...          ...        ...           ...       ...     ...  ...   \n",
       "9995        9996     15606229   Obijiaku           771    France    Male   39   \n",
       "9996        9997     15569892  Johnstone           516    France    Male   35   \n",
       "9997        9998     15584532        Liu           709    France  Female   36   \n",
       "9998        9999     15682355  Sabbatini           772   Germany    Male   42   \n",
       "9999       10000     15628319     Walker           792    France  Female   28   \n",
       "\n",
       "      tenure    balance  num_of_products  has_cr_card  is_active  \\\n",
       "0        2.0       0.00                1            1          1   \n",
       "1        1.0   83807.86                1            0          1   \n",
       "2        8.0  159660.80                3            1          0   \n",
       "3        1.0       0.00                2            0          0   \n",
       "4        2.0  125510.82                1            1          1   \n",
       "...      ...        ...              ...          ...        ...   \n",
       "9995     5.0       0.00                2            1          0   \n",
       "9996    10.0   57369.61                1            1          1   \n",
       "9997     7.0       0.00                1            0          1   \n",
       "9998     3.0   75075.31                2            1          0   \n",
       "9999     NaN  130142.79                1            1          0   \n",
       "\n",
       "      estimated_salary  exited  \n",
       "0            101348.88       1  \n",
       "1            112542.58       0  \n",
       "2            113931.57       1  \n",
       "3             93826.63       0  \n",
       "4             79084.10       0  \n",
       "...                ...     ...  \n",
       "9995          96270.64       0  \n",
       "9996         101699.77       0  \n",
       "9997          42085.58       1  \n",
       "9998          92888.52       1  \n",
       "9999          38190.78       0  \n",
       "\n",
       "[10000 rows x 14 columns]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Al parecer lo que nos toca ahora es simplemente transformar las columnas `geography` y `gender` en formato OHE ya que las columnas que podriamos calificar como Ordinal ya estan de esa forma. Para nuestra suerte, pandas tiene una función que nos permite hacer éste cambio muy fácil: `pd.get_dummies()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definimos las columnas que vamos a transformar\n",
    "dummy_columns = ['geography', 'gender']\n",
    "\n",
    "# Pero tenemos que aclarar el parámetro drop_first para no caer en la trampa dummy\n",
    "filled_data = pd.get_dummies(filled_data, columns= dummy_columns, drop_first= True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Y no nos olvidemos de la tabla limpia\n",
    "clean_data = pd.get_dummies(clean_data, columns= dummy_columns, drop_first= True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='toc1_4_'></a>[Evaluación de balance de clases](#toc0_)\n",
    "  \n",
    "Ahora nos vamos a centrar en ver si las diferentes clases estan muy desbalanceadas o no. Para nuestra suerte, podemos generalizar el trabajo tanto para `filled_data` como para `clean_data` ya que vimos que presentan distribuciones muy similares."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lo primero que vamos a hacer es dterminar las columnas que no nos sirven para entrenar\n",
    "useless_columns = ['row_number','customer_id','surname']\n",
    "\n",
    "# De ahi guardamos en variables nuevas solo con datos para entrenar\n",
    "useful_filled_data = filled_data.drop(columns= useless_columns)\n",
    "useful_clean_data = clean_data.drop(columns= useless_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>credit_score</th>\n",
       "      <th>age</th>\n",
       "      <th>tenure</th>\n",
       "      <th>balance</th>\n",
       "      <th>num_of_products</th>\n",
       "      <th>has_cr_card</th>\n",
       "      <th>is_active</th>\n",
       "      <th>estimated_salary</th>\n",
       "      <th>exited</th>\n",
       "      <th>geography_Germany</th>\n",
       "      <th>geography_Spain</th>\n",
       "      <th>gender_Male</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>619</td>\n",
       "      <td>42</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>101348.88</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>608</td>\n",
       "      <td>41</td>\n",
       "      <td>1.0</td>\n",
       "      <td>83807.86</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>112542.58</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>502</td>\n",
       "      <td>42</td>\n",
       "      <td>8.0</td>\n",
       "      <td>159660.80</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>113931.57</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>699</td>\n",
       "      <td>39</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>93826.63</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>850</td>\n",
       "      <td>43</td>\n",
       "      <td>2.0</td>\n",
       "      <td>125510.82</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>79084.10</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9995</th>\n",
       "      <td>771</td>\n",
       "      <td>39</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>96270.64</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9996</th>\n",
       "      <td>516</td>\n",
       "      <td>35</td>\n",
       "      <td>10.0</td>\n",
       "      <td>57369.61</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>101699.77</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9997</th>\n",
       "      <td>709</td>\n",
       "      <td>36</td>\n",
       "      <td>7.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>42085.58</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9998</th>\n",
       "      <td>772</td>\n",
       "      <td>42</td>\n",
       "      <td>3.0</td>\n",
       "      <td>75075.31</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>92888.52</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9999</th>\n",
       "      <td>792</td>\n",
       "      <td>28</td>\n",
       "      <td>5.0</td>\n",
       "      <td>130142.79</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>38190.78</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10000 rows × 12 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      credit_score  age  tenure    balance  num_of_products  has_cr_card  \\\n",
       "0              619   42     2.0       0.00                1            1   \n",
       "1              608   41     1.0   83807.86                1            0   \n",
       "2              502   42     8.0  159660.80                3            1   \n",
       "3              699   39     1.0       0.00                2            0   \n",
       "4              850   43     2.0  125510.82                1            1   \n",
       "...            ...  ...     ...        ...              ...          ...   \n",
       "9995           771   39     5.0       0.00                2            1   \n",
       "9996           516   35    10.0   57369.61                1            1   \n",
       "9997           709   36     7.0       0.00                1            0   \n",
       "9998           772   42     3.0   75075.31                2            1   \n",
       "9999           792   28     5.0  130142.79                1            1   \n",
       "\n",
       "      is_active  estimated_salary  exited  geography_Germany  geography_Spain  \\\n",
       "0             1         101348.88       1                  0                0   \n",
       "1             1         112542.58       0                  0                1   \n",
       "2             0         113931.57       1                  0                0   \n",
       "3             0          93826.63       0                  0                0   \n",
       "4             1          79084.10       0                  0                1   \n",
       "...         ...               ...     ...                ...              ...   \n",
       "9995          0          96270.64       0                  0                0   \n",
       "9996          1         101699.77       0                  0                0   \n",
       "9997          1          42085.58       1                  0                0   \n",
       "9998          0          92888.52       1                  1                0   \n",
       "9999          0          38190.78       0                  0                0   \n",
       "\n",
       "      gender_Male  \n",
       "0               0  \n",
       "1               0  \n",
       "2               0  \n",
       "3               0  \n",
       "4               0  \n",
       "...           ...  \n",
       "9995            1  \n",
       "9996            1  \n",
       "9997            0  \n",
       "9998            1  \n",
       "9999            0  \n",
       "\n",
       "[10000 rows x 12 columns]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "useful_filled_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    0.796062\n",
       "1    0.203938\n",
       "Name: exited, dtype: float64"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Primero veamos la distribucion de clases de exited en useful_clean_data\n",
    "useful_clean_data['exited'].value_counts(normalize= True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    0.7963\n",
       "1    0.2037\n",
       "Name: exited, dtype: float64"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Veamos entonces la distribución de clases de exited\n",
    "useful_filled_data['exited'].value_counts(normalize= True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Claramente eso no es una clase balanceada. Lo que vamos a hacer ahora es trabajar con las tablas de tal manera que para el final de ésta etapa terminemos con 3 variables importantes:\n",
    "- La tabla de datos tras un proceso de sobremuestreo\n",
    "- La tabla de datos tras un proceso de submuestreo\n",
    "- La tabla de datos sin procesar el desbalance de clases\n",
    "  \n",
    "Posteriormente evaluaremos cual de las tablas tiene un mejor rendimiento para el modelo que elijamos y con esa información conocida haremos el balance de clases usado para la tabla de `useful_clean_data`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='toc1_4_1_'></a>[Sobremuestreo](#toc0_)\n",
    "  \n",
    "Para efectuar el sobremuestreo primero vamos a crear una función que lo haga por nosotros. Aunque antes de eso, debemos separar los datos en *features y target*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Guardamos las features\n",
    "filled_features = useful_filled_data.drop(columns= ['exited'])\n",
    "filled_target = useful_filled_data['exited']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def upsample(features, target, repeat):\n",
    "    \n",
    "    # Primero separamos los 0 y 1 de target\n",
    "    features_zeros = features[target == 0]\n",
    "    features_ones = features[target == 1]\n",
    "    target_zeros = target[target == 0]\n",
    "    target_ones = target[target == 1]\n",
    "    \n",
    "    # Posteriormente aumentamos la cantidad de 0 en los datos\n",
    "    features_upsampled = pd.concat([features_zeros] + [features_ones] * repeat)\n",
    "    target_upsampled = pd.concat([target_zeros] + [target_ones] * repeat)\n",
    "    \n",
    "    # Finalmente mezclamos todo\n",
    "    features_upsampled, target_upsampled = shuffle(\n",
    "    features_upsampled, target_upsampled, random_state=46587)\n",
    "    \n",
    "    # Y devolvemos las variables \n",
    "    return features_upsampled, target_upsampled\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtengamos entonces los datos sobremuestreados (eso es una palabra?)\n",
    "upsamp_ffeatures, upsamp_ftarget = upsample(filled_features, filled_target, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(16111, 11)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1    0.505741\n",
       "0    0.494259\n",
       "Name: exited, dtype: float64"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Y veamos como quedó el balance\n",
    "print(upsamp_ffeatures.shape)\n",
    "upsamp_ftarget.value_counts(normalize= True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vemos que logramos balancear las clases bastante bien y aumentamos las filas totales en 6111 lo cual no es poco, pero el desbalance presentado lo requería.\n",
    "  \n",
    "### <a id='toc1_4_2_'></a>[Submuestreo](#toc0_)\n",
    "  \n",
    "Ahora vamos a seguir un procedimiento muy similar al que tuvimos previamente pero al revés. Mi predicción es que éste balance nos hará perder una buena porción de los datos pero vamos a ello."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def downsample(features, target, fraction):\n",
    "    # Separamos una vez más los datos en 0 y 1\n",
    "    features_zeros = features[target == 0]\n",
    "    features_ones = features[target == 1]\n",
    "    target_zeros = target[target == 0]\n",
    "    target_ones = target[target == 1]\n",
    "    \n",
    "    # Para reducir los datos tenemos que llamar a una funcion de pandas\n",
    "    features_downsampled = pd.concat(\n",
    "    [features_zeros.sample(frac=fraction, random_state=54321)] + [features_ones])\n",
    "    \n",
    "    target_downsampled = pd.concat(\n",
    "    [target_zeros.sample(frac=fraction, random_state=54321)] + [target_ones])\n",
    "    \n",
    "    # Finalmente mezclamos los datos\n",
    "    features_downsampled, target_downsampled = shuffle(\n",
    "    features_downsampled, target_downsampled, random_state=54321)\n",
    "    \n",
    "    # Y devolemos las tablas ya submuestreadas\n",
    "    return features_downsampled, target_downsampled\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtenemos los datos tras submuestrearlos (otra palabra que desconozco su existencia)\n",
    "downsamp_ffeatures, downsamp_ftarget = downsample(filled_features, filled_target, 0.256123)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4077, 11)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0    0.500368\n",
       "1    0.499632\n",
       "Name: exited, dtype: float64"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Y verificamos su distribución\n",
    "print(downsamp_ffeatures.shape)\n",
    "downsamp_ftarget.value_counts(normalize= True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La ventaja que tenemos en el downsample es que toma valores flotantes por lo que podemos lograr un balance mucho más preciso aunque jamas lograremos llegar a ese 50/50 deseado. La desventaja es que perdimos casi 6000 datos y como sabemos, mientras más datos tengamos para nuestro modelo, mejor.\n",
    "  \n",
    "Por lo tanto, terminamos con éstos tres conjuntos de datos:\n",
    "- `upsamp_ffeatures` y `upsamp_ftarget`: Los datos tras un balance basado en el sobremuestreo\n",
    "- `downsamp_ffeatures` y `downsamp_ftarget`: Los datos tras un balance basado en el submuestreo\n",
    "- `filled_features` y `filled_target`: Los datos desbalanceados\n",
    "  \n",
    "En la siguientes etapas nos dedicaremos a segmentar los datos, buscar los mejores modelos para nuestros datos y elegir un conjunto definitivo sobre el cual llevaremos a cabo las pruebas."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='toc1_5_'></a>[Segmentación de datos](#toc0_)\n",
    "  \n",
    "Vamos a usar la función `GridSearchCV` para la optimización de hiperparámetros la que nos permitirá reducir el sobreajuste de nuestros modelos. Para lograr de mejor manera eso, vamos a dividir los datos en 2 partes: entrenamiento (75%) y pruebas (25%).\n",
    "\n",
    "De no usar `GridSearchCV` si habría que separar el dataset en 3 partes para tener un conjunto de validación lo que como consecuencia reduciría el tamaño total del conjunto de entrenamiento."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ahora vamos a separar los grupos de los 3 conjuntos de datos que mecionamos previamente\n",
    "# Primero los datos sobremuestreados (UPSampled)\n",
    "ups_ffeatures_train, ups_ffeatures_test, ups_ftarget_train, ups_ftarget_test = train_test_split(\n",
    "    upsamp_ffeatures, upsamp_ftarget, test_size=0.25, random_state=45695)\n",
    "\n",
    "# De ahi los submuestreados (DoWnSampled)\n",
    "dws_ffeatures_train, dws_ffeatures_test, dws_ftarget_train, dws_ftarget_test = train_test_split(\n",
    "    downsamp_ffeatures, downsamp_ftarget, test_size=0.25, random_state=45695)\n",
    "\n",
    "# Finalmente los datos desbalanceados (UNBalanced)\n",
    "unb_ffeatures_train, unb_ffeatures_test, unb_ftarget_train, unb_ftarget_test = train_test_split(\n",
    "    filled_features, filled_target, test_size=0.25, random_state=45695)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Me parece que antes de seguir voy a aclarar bien la nomenclatura de todas esas variables. Antes de ir a esas variables, veamos de vuelta que tenemos antes:\n",
    "- `upsamp_ffeatures` y `upsamp_ftarget`: Los datos tras un balance basado en el sobremuestreo\n",
    "- `downsamp_ffeatures` y `downsamp_ftarget`: Los datos tras un balance basado en el submuestreo\n",
    "- `filled_features` y `filled_target`: Los datos desbalanceados\n",
    "- `useful_clean_data`: La tabla sin separar en features o targets a la que se le eliminaron las filas con valores nulos\n",
    "    \n",
    "Es por eso que la estructura general de los nombres tiene 3 partes para 4 cosas:\n",
    "  \n",
    "`[A]`_`[B]` `[C]`_`[D]`\n",
    "- **A:** Sobremuestreados (ups) / Submuestreados (dws) / Desbalanceados (unb)\n",
    "- **B:** Datos imputados (f) / Limpios (c)\n",
    "- **C:** Features (features) / Target (target)\n",
    "- **D:** De prueba (train) / De entrenameiento (test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='toc1_6_'></a>[Prueba de modelos e hiperparámetros](#toc0_)\n",
    "  \n",
    "En éste caso nos encontramos con una problemática de clasificación por lo que vamos a probar 2 modelos:\n",
    "\n",
    "- Modelo de bosque alteatorio (RandomForestClassifier)\n",
    "- Modelo de regresión logística (LogisticRegression)\n",
    "  \n",
    "Como dijimos, vamos a usar `GridSearchCV` para obtener la mejor combinación de hiperparámetros acorde a nuestros datos y a los modelos en cuestión.\n",
    "  \n",
    "Ésta etapa será algo tediosa ya que tendremos que entrenar 3 modelos para determinar cuál es el mejor."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> Disclaimer </h1>\n",
    "  \n",
    "**Al hacer el proyecto yo usé la función GridSearchCV pero ésta se toma mucho tiempo por el amplio rango de parámetros que elegí. Ante ésto yo copié el modelo optimizado y lo guardé en la variable. Todo el código original relacionado a la función GridSearchCV está comentado tal y como lo usé en el proyecto con los fines de no tener que esperar tanto para ejecutar el mismo.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='toc1_6_1_'></a>[Modelo de bosque aleatorio](#toc0_)\n",
    "  \n",
    "Un modelo muy capaz pero con un coste computacional muy alto tambien. Como dijimos, vamos a tener que entrenar 3 modelos diferentes para poder compararlos y ver cuál de los dos métodos de balance que usamos es mejor, si es que alguno es mejor."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <a id='toc1_6_1_1_'></a>[Conjunto de datos - Sobremuestreo](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Primero determinaremos sobre cuales hiperparámetros vamos a trabajar\n",
    "params = {\n",
    "    'n_estimators': [50,75,100],\n",
    "    'criterion': ['entropy'],\n",
    "    'max_depth': list(range(2,7)),\n",
    "    'min_samples_split': list(range(2,10)),\n",
    "    'min_samples_leaf': list(range(1,6)),\n",
    "    'max_features': ['sqrt', 'log2', None],\n",
    "    'random_state': [45695]\n",
    "}\n",
    "\n",
    "# Cargamos el grid en una variable\n",
    "forest_grid = GridSearchCV(RandomForestClassifier(), param_grid= params, cv= 5, verbose= 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Primero buscamos el mejor conjunto de h-parametros para los datos sobremuestreados\n",
    "\n",
    "#forest_grid.fit(ups_ffeatures_train, ups_ftarget_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Veamos cual es el mejor modelo\n",
    "\n",
    "#forest_grid.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Y cargemos ese modelo en una variable\n",
    "#ups_forest_model = forest_grid.best_estimator_\n",
    "\n",
    "ups_forest_model = RandomForestClassifier(criterion='entropy', max_depth=6, max_features=None,\n",
    "                       min_samples_split=3, n_estimators=75,\n",
    "                       random_state=45695)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(criterion='entropy', max_depth=6, max_features=None,\n",
       "                       min_samples_split=3, n_estimators=75,\n",
       "                       random_state=45695)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# De ahi entrenamos el modelo\n",
    "ups_forest_model.fit(ups_ffeatures_train, ups_ftarget_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finalmente guardamos la predicción\n",
    "ups_forest_predict = ups_forest_model.predict(ups_ffeatures_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Valor F1: 0.78506\n",
      " Valor AUC-ROC: 0.77989\n"
     ]
    }
   ],
   "source": [
    "# Ahora veamos el puntaje F1 y el puntaje AUC-ROC\n",
    "print(\n",
    "f' Valor F1: {f1_score(ups_ftarget_test, ups_forest_predict):.5f}'\n",
    "f'\\n Valor AUC-ROC: {roc_auc_score(ups_ftarget_test, ups_forest_predict):.5f}'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Podemos observar que las grid ciertamente lograron una buena combinación de hiperparámetros ya que el puntaje F1 está bastante por encima del umbral requerido. Tambien podemos observar que el valor AUC-ROC es un poco menor que el F1 pero de todas formas es casi 0,8. Al parecer el sobremuestreo junto con los hiperparámetros obtenidos con `GridSearchCV` obtienen un muy buen resultado.\n",
    "    \n",
    "Pasemos entonces a ver como se comportan el resto de los datos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <a id='toc1_6_1_2_'></a>[Conjunto de datos - Submuestreo](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Determinaremos sobre cuales hiperparámetros vamos a trabajar\n",
    "params = {\n",
    "    'n_estimators': [50,75,100],\n",
    "    'criterion': ['entropy'],\n",
    "    'max_depth': list(range(2,7)),\n",
    "    'min_samples_split': list(range(2,10)),\n",
    "    'min_samples_leaf': list(range(1,6)),\n",
    "    'max_features': ['sqrt', 'log2', None],\n",
    "    'random_state': [45695]\n",
    "}\n",
    "\n",
    "# Cargamos el grid en una variable\n",
    "forest_grid = GridSearchCV(RandomForestClassifier(), param_grid= params, cv= 5, verbose= 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Primero buscamos el mejor conjunto de h-parametros para los datos sobremuestreados\n",
    "\n",
    "#forest_grid.fit(dws_ffeatures_train, dws_ftarget_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Veamos cual es el mejor modelo\n",
    "\n",
    "# forest_grid.best_estimator_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Algo que quiero destacar es como el conjunto de hiperparámetros para éstos datos es difertente al de los sobremuestreados. Aunque tiene sentido ya que tratamos con diferentes datos.\n",
    "  \n",
    "Me llama la atención como `max_features` acá es la raiz cuadrada de los totales cuando en el otro modelo no había un límite. También notamos como éste bosque tiene 25 árboles menos. Aunque... el `min_samples_split` es el doble... al parecer éstos árboles tienen ramas \"más duras\"! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Y cargemos ese modelo en una variable\n",
    "#dws_forest_model = forest_grid.best_estimator_\n",
    "\n",
    "dws_forest_model = RandomForestClassifier(criterion='entropy', max_depth=6, max_features='sqrt',\n",
    "                       min_samples_split=6, n_estimators=50,\n",
    "                       random_state=45695)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(criterion='entropy', max_depth=6, max_features='sqrt',\n",
       "                       min_samples_split=6, n_estimators=50,\n",
       "                       random_state=45695)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# De ahi entrenamos el modelo\n",
    "dws_forest_model.fit(dws_ffeatures_train, dws_ftarget_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finalmente guardamos la predicción\n",
    "dws_forest_predict = dws_forest_model.predict(dws_ffeatures_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Valor F1: 0.73504\n",
      " Valor AUC-ROC: 0.75479\n"
     ]
    }
   ],
   "source": [
    "# Ahora veamos el puntaje F1 y el puntaje AUC-ROC\n",
    "print(\n",
    "f' Valor F1: {f1_score(dws_ftarget_test, dws_forest_predict):.5f}'\n",
    "f'\\n Valor AUC-ROC: {roc_auc_score(dws_ftarget_test, dws_forest_predict):.5f}'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Al parecer el submuestreo no es la mejor de las dos técnicas de balance para nuestro caso, no es que éste sea una mala opción pues ~0.75 no es un valor para despreciar! Y vemos como el modelo logra en promedio un 76,7% verdaderos positivos gracias al puntaje AUC-ROC."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <a id='toc1_6_1_3_'></a>[Conjunto de datos - Desbalance de clases](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Determinaremos sobre cuales hiperparámetros vamos a trabajar\n",
    "params = {\n",
    "    'n_estimators': [50,75,100],\n",
    "    'criterion': ['entropy'],\n",
    "    'max_depth': list(range(2,7)),\n",
    "    'min_samples_split': list(range(2,10)),\n",
    "    'min_samples_leaf': list(range(1,6)),\n",
    "    'max_features': ['sqrt', 'log2', None],\n",
    "    'random_state': [45695]\n",
    "}\n",
    "\n",
    "# Cargamos el grid en una variable\n",
    "forest_grid = GridSearchCV(RandomForestClassifier(), param_grid= params, cv= 5, verbose= 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Primero buscamos el mejor conjunto de h-parametros para los datos sobremuestreados\n",
    "\n",
    "# forest_grid.fit(unb_ffeatures_train, unb_ftarget_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Veamos cual es el mejor modelo\n",
    "\n",
    "# forest_grid.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Y cargemos ese modelo en una variable\n",
    "#unb_forest_model = forest_grid.best_estimator_\n",
    "\n",
    "unb_forest_model = RandomForestClassifier(criterion='entropy', max_depth=6, max_features=None,\n",
    "                       min_samples_leaf=5, n_estimators=75, random_state=45695)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(criterion='entropy', max_depth=6, max_features=None,\n",
       "                       min_samples_leaf=5, n_estimators=75, random_state=45695)"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# De ahi entrenamos el modelo\n",
    "unb_forest_model.fit(unb_ffeatures_train, unb_ftarget_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finalmente guardamos la predicción\n",
    "unb_forest_predict = unb_forest_model.predict(unb_ffeatures_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Valor F1: 0.55456\n",
      " Valor AUC-ROC: 0.70267\n"
     ]
    }
   ],
   "source": [
    "# Ahora veamos el puntaje F1 y el puntaje AUC-ROC\n",
    "print(\n",
    "f' Valor F1: {f1_score(unb_ftarget_test, unb_forest_predict):.5f}'\n",
    "f'\\n Valor AUC-ROC: {roc_auc_score(unb_ftarget_test, unb_forest_predict):.5f}'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Efectivamente vemos porque hay que balancear los datos! Éste modelo ni llega a pasar el umbral que debiamos tratar... Y pensar que ese modelo tiene una muy buena combinación de hiperparámetros! Por otra parte el valor AUC-ROC se ve notoriamente alto, pero eso se debe a que dicho valor no es óptimo para conjuntos de datos inbalanceados.\n",
    "  \n",
    "Por lo tanto, el ganador dentro de la categoría es el modelo con los datos balanceados mediante sobremuestreo! Con un valor f1 de 0,78506 y un puntaje AUC-ROC de 0,77989. Ya despues analizaremos bien que puede haber causado la diferencia entre los dos conjuntos de datos balanceados."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='toc1_6_2_'></a>[Modelo de regresión logística](#toc0_)\n",
    "  \n",
    "Un método muy diferente al de árboles de decisión ya que la forma de obtener la respuesta es formando una función matemática y en base a eso determina el resultado para una dada característica.\n",
    "  \n",
    "Por supuesto, vamos a usar una vez más el `GridSearchCV` para determinar la mejor combinación de hiperparámetros para nuestros datos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <a id='toc1_6_2_1_'></a>[Conjunto de datos - Sobremuestreo](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Primero vamos a determinar los h-parámetros que vamos a probar\n",
    "params = {\n",
    "    'C': np.logspace(-4,4,35),\n",
    "    'solver': ['lbfgs', 'liblinear'],\n",
    "    'max_iter': list(range(100,310,50)),\n",
    "    'random_state': [45695]\n",
    "}\n",
    "\n",
    "logistic_grid = GridSearchCV(LogisticRegression(), param_grid= params, cv= 10, verbose= 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ahora determinamos los h-parámetros\n",
    "\n",
    "# logistic_grid.fit(ups_ffeatures_train, ups_ftarget_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Entonces, cual es la mejor combincación?\n",
    "# logistic_grid.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cargamos el modelo en una variable\n",
    "#ups_logistic_model = logistic_grid.best_estimator_\n",
    "\n",
    "ups_logistic_model = LogisticRegression(C=0.0015013107289081743, random_state=45695,\n",
    "                   solver='liblinear')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=0.0015013107289081743, random_state=45695,\n",
       "                   solver='liblinear')"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Lo entrenamos\n",
    "ups_logistic_model.fit(ups_ffeatures_train, ups_ftarget_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Guardamos su predicción\n",
    "ups_logistic_predict = ups_logistic_model.predict(ups_ffeatures_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Valor F1: 0.67486\n",
      " Valor AUC-ROC: 0.66461\n"
     ]
    }
   ],
   "source": [
    "# Y finalmente vemos sus puntajes!\n",
    "print(\n",
    "f' Valor F1: {f1_score(ups_ftarget_test, ups_logistic_predict):.5f}'\n",
    "f'\\n Valor AUC-ROC: {roc_auc_score(ups_ftarget_test, ups_logistic_predict):.5f}'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Al parecer nuestro modelo logístico no es un mal modelo tampoco! Con un puntaje F1 de 0,67 ciertamente es peor que su equivalente de bosque aleatorio pero si tomamos en cuenta el costo computacional de ambos éste es mejor. Tambien notamos como el puntaje AUC-ROC tambien se encuentra un poco por detras del F1 similar a lo que vimos en el bosque aleatorio.\n",
    "  \n",
    "Pasemos entonces a entrenar el modelo con datos submuestreados para comparar como se comporta el modelo logístico frente al modelo de bosques aleatorios."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <a id='toc1_6_2_2_'></a>[Conjunto de datos - Submuestreo](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Primero vamos a determinar los h-parámetros que vamos a probar\n",
    "params = {\n",
    "    'C': np.logspace(-4,4,35),\n",
    "    'solver': ['lbfgs', 'liblinear'],\n",
    "    'max_iter': list(range(100,310,50)),\n",
    "    'random_state': [45695]\n",
    "}\n",
    "\n",
    "logistic_grid = GridSearchCV(LogisticRegression(), param_grid= params, cv= 10, verbose= 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ahora determinamos los h-parámetros\n",
    "\n",
    "#logistic_grid.fit(dws_ffeatures_train, dws_ftarget_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Entonces, cual es la mejor combincación?\n",
    "\n",
    "# logistic_grid.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cargamos el modelo en una variable\n",
    "#dws_logistic_model = logistic_grid.best_estimator_\n",
    "\n",
    "dws_logistic_model = LogisticRegression(C=0.11450475699382812, random_state=45695,\n",
    "                   solver='liblinear')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=0.11450475699382812, random_state=45695,\n",
       "                   solver='liblinear')"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Lo entrenamos\n",
    "dws_logistic_model.fit(dws_ffeatures_train, dws_ftarget_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Guardamos su predicción\n",
    "dws_logistic_predict = dws_logistic_model.predict(dws_ffeatures_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Valor F1: 0.65121\n",
      " Valor AUC-ROC: 0.66072\n"
     ]
    }
   ],
   "source": [
    "# Y finalmente vemos sus puntajes!\n",
    "print(\n",
    "f' Valor F1: {f1_score(dws_ftarget_test, dws_logistic_predict):.5f}'\n",
    "f'\\n Valor AUC-ROC: {roc_auc_score(dws_ftarget_test, dws_logistic_predict):.5f}'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Al parecer vemos una situación muy similar a la del bosque aleatorio ya que el conjunto sobremuestreado presenta de vuelta tanto un valor F1 como un AUC-ROC un poco mayor al submuestreado, e incluso podemos notar como en los dos pares de casos el conjunto submuestreado presenta un valor AUC-ROC ligeramente superior al F1. \n",
    "  \n",
    "Ésto último nos indica algo respecto al comportamiento de los datos ante el sobremuestreo y el submuestreo. Mi hipótesis es que si bien estamos trabajando con 10000 filas, al parecer reducir la cantidad total de filas para el entrenamiento tiene una ligera desventaja ya que terminamos con menos de la mitad y eso repercute ligeramente sobre la calidad de un modelo dado que tiene menos datos sobre los cuales entrenarse."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <a id='toc1_6_2_3_'></a>[Conjunto de datos - Desbalance](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Primero vamos a determinar los h-parámetros que vamos a probar\n",
    "params = {\n",
    "    'C': np.logspace(-4,4,35),\n",
    "    'solver': ['lbfgs', 'liblinear'],\n",
    "    'max_iter': list(range(100,310,50)),\n",
    "    'random_state': [45695]\n",
    "}\n",
    "\n",
    "logistic_grid = GridSearchCV(LogisticRegression(), param_grid= params, cv= 10, verbose= 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ahora determinamos los h-parámetros\n",
    "\n",
    "# logistic_grid.fit(unb_ffeatures_train, unb_ftarget_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Entonces, cual es la mejor combincación?\n",
    "\n",
    "# logistic_grid.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cargamos el modelo en una variable\n",
    "#unb_logistic_model = logistic_grid.best_estimator_\n",
    "\n",
    "unb_logistic_model = LogisticRegression(C=0.0001, random_state=45695, solver='liblinear')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=0.0001, random_state=45695, solver='liblinear')"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Lo entrenamos\n",
    "unb_logistic_model.fit(unb_ffeatures_train, unb_ftarget_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Guardamos su predicción\n",
    "unb_logistic_predict = unb_logistic_model.predict(unb_ffeatures_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Valor F1: 0.10450\n",
      " Valor AUC-ROC: 0.52027\n"
     ]
    }
   ],
   "source": [
    "# Y finalmente vemos sus puntajes!\n",
    "print(\n",
    "f' Valor F1: {f1_score(unb_ftarget_test, unb_logistic_predict):.5f}'\n",
    "f'\\n Valor AUC-ROC: {roc_auc_score(unb_ftarget_test, unb_logistic_predict):.5f}'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Una vez más podemos notar el impacto de entrenar un modelo desbalanceado ya que éste modelo se comporta mucho peor que tirar una moneda!\n",
    "  \n",
    "Sin más vueltas, pasemos a la conclusión final sobre cual modelo es el mejor!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='toc1_7_'></a>[Prueba final](#toc0_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Antes de pasar en sí a la prueba final, recordemos los puntajes de los diferentes modelos:\n",
    "\n",
    "| Modelo | Sobremuestreo (F1) | Sobremuestreo (AUC-ROC) | Submuestreo (F1) | Submuestreo (AUC-ROC) |\n",
    "|----------|----------|----------|----------|----------|\n",
    "|Bosque Aleatorio|0.78506|0.77989|0.73504|0.75479|\n",
    "|Regresión Logística|0.67486|0.66461|0.65121|0.66072|\n",
    "  \n",
    "Como podemos ver, el modelo de Bosque Aleatorio de Decisiones obtuvo mejores métricas en todos los conjuntos y los conjuntos sobremuestreados obtuvieron mejores resultados en ambos modelos que los submuestreados.\n",
    "  \n",
    "En el primer caso es evidente ya que el modelo de Bosque aleatorio es un modelo mucho más complejo que logra adaptarse mucho mejor a las diferentes conexiones que hay entre las features para deducir el resultado mientras que la regresión logística si bien también \"aprende\" de todas las features, tambien las considera a todas a la vez mediante la fórmula matemáteica que la define. Un detalle no menor, el tiempo de ejecución de la Regresión es mucho menor al de los Bosques Aleatorios y ambos modelos pasan el umbral estipulado al comienzo.\n",
    "  \n",
    "Por lo tanto, nosotros nos vamos a quedar con el modelo de Bosque Aleatorio con datos sobremuestreados! Entonces pasemos a la prueba final."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Primero guardamos el modelo de bosque aleatorio en una variable para no perderlo\n",
    "final_model = ups_forest_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "# De ahi vamos a mezclar todo el dataset para realizar la prueba final con todo el conjunto, usamos un nuevo random_state\n",
    "# para intentar simular un conjunto completamente nuevo\n",
    "final_features, final_target = filled_features, filled_target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vemos que predice el modelo\n",
    "final_predict = final_model.predict(final_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Valor F1: 0.59657\n",
      " Valor AUC-ROC: 0.78791\n"
     ]
    }
   ],
   "source": [
    "# Entonces veamos el puntaje final!!\n",
    "print(\n",
    "f' Valor F1: {f1_score(final_target, final_predict):.5f}'\n",
    "f'\\n Valor AUC-ROC: {roc_auc_score(final_target, final_predict):.5f}'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bueno... esos... no son los resultados que esperaba. Técnicamente superamos el umbral requerido pero no puedo evitar notar la gran diferencia en puntajes que vimos hasta ahora. Lo más probable es que el problema nazca de que el modelo se haya acostumbrado a las \"personas imaginarias\" que creamos a partir del sobremuestreo y que su \"tren de pensamiento\" presente un sesgo debido a que hicimos parecer normal algo que no era. Basicamente, el modelo vió que la gran mayoría de los que se iban del banco presentaban las mismas características hasta el más minimo detalle y eso seguro causó que busque patrones que no necesariamente se correlacionan o capaz que hizo una suerte de sobreajuste en la que los márgenes para variables numéricas eran muy chicos y no daban margen para otros casos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='toc1_8_'></a>[Conclusión](#toc0_)\n",
    "  \n",
    "Después de muchas pruebas, mucho entrenamiento y varios resultados finalmente llegamos a la conclusión! Vamos por partes:\n",
    "  \n",
    "1. **Los datos**\n",
    "  \n",
    "La tabla que nos dieron para realizar el trabajo estaban en una condición casi perfecta, no hubo que corregir los tipos de las columnas, no presentaban valores duplicados y solo un ~10% presentaban valores ausentes y en solo una columna. No puedo apuntar a un motivo concreto sobre el que pueda justificar la presencia de esos valores ausentes pues sus distribuciones iban en concordancia con el resto del dataframe por lo que creo que es mejor estudiar todo el camino que pasaron esos datos para encontrar la causa más que buscar un patrón en su origen o características.\n",
    "  \n",
    "Por otra parte, me parece que la tabla si merece un análisis riguroso ya que nuestra busqueda superficial ya empezó a revelar múltiples factores en común sobre los que vale la pena ahondar e investigar (como que la mayoria de los clientes que se van tienen solo 1 producto). Mi recomendación sería pasar la tabla a un Data Analyst ya que si bien un modelo de Machine Learning puede darnos una respuesta de quién se está por ir, no nos va a dar la solución sobre que debemos hacer para ese caso. En cambio, con un análisis más profundo podriamos descubrir no solo las causas detras de los clientes yendose sino también unas soluciones aptas para cada situación.\n",
    "  \n",
    "Finalmente hay que nombrar que antes de poder trabajar con nuestros datos respecto a Machine Learning tuvimos que transformar las columnas `gender` y `geography` con la metodología One-Hot Coding ya que no podíamos aplicar el método ordinal porque no queremos hacerle creer que los países o los géneros poseen una relación en la que alguno es mayor o menor.\n",
    "  \n",
    "2. **Desbalance de datos**\n",
    " \n",
    "Si bien los datos se encontraban relativamente limpios, no podemos decir lo mismo de la distribución de los mismos ya que encontramos como el 79% de los mismos no se habían ido del banco. Para eso probamos 2 técnicas diferentes de balance: el sobremuestreo y el submuestreo. Como vimos antes, consiste en repetir/eliminar las filas del conjunto que es más raro/común para así lograr un balance y llevar la proporcion a un ratio 1:1.\n",
    "  \n",
    "Honestamente si tienen sus detalles ambos métodos ya que es elegir uno de dos caminos: O repetimos datos y creamos personas imaginarias que coinciden hasta el último detalle con otras, o eliminamos datos que tienen el potencial de proporcionar nuevos conocimientos sobre la relación entre las features y los datos. Ya hablaremos un poco más de eso más adelante.\n",
    "  \n",
    "3. **Entrenamiento de modelos**\n",
    "  \n",
    "De ahi pasamos al entrenamiento de modelos, entrenamos un total de 6 modelos diferentes de los cuales 3 eran bosque aleatorio y 3 eran de regresión logística. Eran 3 de cada uno ya que quería evaluar los comportamientos frente a de los conjuntos sin balancear. Fue más que evidente que los modelos entrenados sin balancear los datos terminaban mucho peor ya que obtuvieron un puntaje F1 de 0,55456 en el caso de bosque aleatorio y 0,10450 en el caso de la regresión logística. Mientras tanto, el resto de los modelos demostraron unos valores F1 muy buenos:\n",
    "\n",
    "| Modelo | Sobremuestreo (F1) | Sobremuestreo (AUC-ROC) | Submuestreo (F1) | Submuestreo (AUC-ROC) |\n",
    "|----------|----------|----------|----------|----------|\n",
    "|Bosque Aleatorio|0.78506|0.77989|0.73504|0.75479|\n",
    "|Regresión Logística|0.67486|0.66461|0.65121|0.66072|\n",
    "\n",
    "Estoy usando de vuelta la tabla? Si, es una buena manera de mostrar los valores y poder comparar lado a lado todo. Como dije previamente, el modelo de bosque aleatorio demostró puntajes F1 superiores al de la regresión logística pero con un tiempo de ejecución mayor tambien. Ese ultimo detalle no es menor ya que de no tener las capacidades computacionales para albergar el modelo y que funcione en tiempos coherentes, puede que sea mejor el modelo logístico. \n",
    "  \n",
    "Otra cosa que hay que destacar es que los modelos fueron evaluados principalemente con el valor F1 que es una medida armónica entre la *precision* y el *recall* por lo que dan una proporción balanceada de errores tanto para los negativos como los positivos. Según el plan de los empresarios del banco, un modelo más enfocado en el *recall* o en la *precision* puede ser más óptimo que el planteado acá.\n",
    "  \n",
    "También hay que destacar como los modelos entrenados con datos sobremuestrados presentaron una ligera ventaja sobre los del submuestreo. Eso se puede dar en que dentro de lo que Machine Learning respecta, 10000 no es un numero abismalmente grande de datos y posiblemente esas 4000 filas removidas en el proceso de submuestreo hayan repercutido de manera ligeramente notoria en las métricas de nuestros modelos.\n",
    "  \n",
    "Antes de irme quiero destacar que yo originalmente planeaba poner a prueba como se iba a comportar un modelo entrenado con los datos a los cuales simplemente se les eliminaron las filas con valores ausentes en `tenure` en vez de imputarlos con la media como hice yo. Mi hipótesis es que hubiesen sido ligeramente peor o igual ya que no eran ni 1000 datos faltantes por lo que nuestra imputación no tuvo tanto impacto en la distribución y por ende en los datos."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
